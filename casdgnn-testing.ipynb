{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA-SDGNN Training, Validation, and Testing\n",
    "\n",
    "This notebook demonstrates how to train, validate, and test the CA-SDGNN framework on the Bitcoin Alpha dataset. The workflow includes:\n",
    "- Mounting Google Drive (if using Colab)\n",
    "- Pretraining the model\n",
    "- Fine-tuning the model\n",
    "- Running validation and testing\n",
    "- Analyzing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3710545,
     "status": "ok",
     "timestamp": 1745002192764,
     "user": {
      "displayName": "Abhin B",
      "userId": "18374123182166733868"
     },
     "user_tz": -330
    },
    "id": "E2g__hQVPRSD",
    "outputId": "b41bed4e-e541-49dd-ab00-bb371b6d2dee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Pretraining Loss: 31.4786\n",
      "Epoch [2/500], Pretraining Loss: 30.4657\n",
      "Epoch [3/500], Pretraining Loss: 29.4441\n",
      "Epoch [4/500], Pretraining Loss: 28.9800\n",
      "Epoch [5/500], Pretraining Loss: 28.3410\n",
      "Epoch [6/500], Pretraining Loss: 27.4996\n",
      "Epoch [7/500], Pretraining Loss: 27.0407\n",
      "Epoch [8/500], Pretraining Loss: 26.1458\n",
      "Epoch [9/500], Pretraining Loss: 25.6498\n",
      "Epoch [10/500], Pretraining Loss: 25.1788\n",
      "Epoch [11/500], Pretraining Loss: 24.9631\n",
      "Epoch [12/500], Pretraining Loss: 24.1118\n",
      "Epoch [13/500], Pretraining Loss: 23.7725\n",
      "Epoch [14/500], Pretraining Loss: 23.1672\n",
      "Epoch [15/500], Pretraining Loss: 23.0386\n",
      "Epoch [16/500], Pretraining Loss: 22.8957\n",
      "Epoch [17/500], Pretraining Loss: 22.2958\n",
      "Epoch [18/500], Pretraining Loss: 22.1936\n",
      "Epoch [19/500], Pretraining Loss: 21.9401\n",
      "Epoch [20/500], Pretraining Loss: 21.7944\n",
      "Epoch [21/500], Pretraining Loss: 21.2566\n",
      "Epoch [22/500], Pretraining Loss: 21.1918\n",
      "Epoch [23/500], Pretraining Loss: 21.0761\n",
      "Epoch [24/500], Pretraining Loss: 20.7190\n",
      "Epoch [25/500], Pretraining Loss: 20.5867\n",
      "Epoch [26/500], Pretraining Loss: 20.3053\n",
      "Epoch [27/500], Pretraining Loss: 20.0412\n",
      "Epoch [28/500], Pretraining Loss: 20.0852\n",
      "Epoch [29/500], Pretraining Loss: 19.5878\n",
      "Epoch [30/500], Pretraining Loss: 19.4932\n",
      "Epoch [31/500], Pretraining Loss: 19.2009\n",
      "Epoch [32/500], Pretraining Loss: 19.0176\n",
      "Epoch [33/500], Pretraining Loss: 18.9353\n",
      "Epoch [34/500], Pretraining Loss: 18.7257\n",
      "Epoch [35/500], Pretraining Loss: 18.5164\n",
      "Epoch [36/500], Pretraining Loss: 18.5998\n",
      "Epoch [37/500], Pretraining Loss: 18.4284\n",
      "Epoch [38/500], Pretraining Loss: 18.3474\n",
      "Epoch [39/500], Pretraining Loss: 18.1686\n",
      "Epoch [40/500], Pretraining Loss: 17.9310\n",
      "Epoch [41/500], Pretraining Loss: 17.5354\n",
      "Epoch [42/500], Pretraining Loss: 17.7630\n",
      "Epoch [43/500], Pretraining Loss: 17.4557\n",
      "Epoch [44/500], Pretraining Loss: 17.5007\n",
      "Epoch [45/500], Pretraining Loss: 17.2389\n",
      "Epoch [46/500], Pretraining Loss: 17.3952\n",
      "Epoch [47/500], Pretraining Loss: 17.1223\n",
      "Epoch [48/500], Pretraining Loss: 17.0316\n",
      "Epoch [49/500], Pretraining Loss: 16.8998\n",
      "Epoch [50/500], Pretraining Loss: 16.7030\n",
      "Epoch [51/500], Pretraining Loss: 16.7099\n",
      "Epoch [52/500], Pretraining Loss: 16.6196\n",
      "Epoch [53/500], Pretraining Loss: 16.4960\n",
      "Epoch [54/500], Pretraining Loss: 16.5117\n",
      "Epoch [55/500], Pretraining Loss: 16.4402\n",
      "Epoch [56/500], Pretraining Loss: 16.4300\n",
      "Epoch [57/500], Pretraining Loss: 16.2035\n",
      "Epoch [58/500], Pretraining Loss: 16.0486\n",
      "Epoch [59/500], Pretraining Loss: 16.0112\n",
      "Epoch [60/500], Pretraining Loss: 15.8894\n",
      "Epoch [61/500], Pretraining Loss: 15.8024\n",
      "Epoch [62/500], Pretraining Loss: 15.7899\n",
      "Epoch [63/500], Pretraining Loss: 15.6659\n",
      "Epoch [64/500], Pretraining Loss: 15.5636\n",
      "Epoch [65/500], Pretraining Loss: 15.5501\n",
      "Epoch [66/500], Pretraining Loss: 15.5216\n",
      "Epoch [67/500], Pretraining Loss: 15.4049\n",
      "Epoch [68/500], Pretraining Loss: 15.2144\n",
      "Epoch [69/500], Pretraining Loss: 15.2572\n",
      "Epoch [70/500], Pretraining Loss: 15.0910\n",
      "Epoch [71/500], Pretraining Loss: 15.1376\n",
      "Epoch [72/500], Pretraining Loss: 14.9823\n",
      "Epoch [73/500], Pretraining Loss: 14.8605\n",
      "Epoch [74/500], Pretraining Loss: 14.6808\n",
      "Epoch [75/500], Pretraining Loss: 14.5873\n",
      "Epoch [76/500], Pretraining Loss: 14.6016\n",
      "Epoch [77/500], Pretraining Loss: 14.5731\n",
      "Epoch [78/500], Pretraining Loss: 14.5847\n",
      "Epoch [79/500], Pretraining Loss: 14.6157\n",
      "Epoch [80/500], Pretraining Loss: 14.3807\n",
      "Epoch [81/500], Pretraining Loss: 14.2549\n",
      "Epoch [82/500], Pretraining Loss: 14.2506\n",
      "Epoch [83/500], Pretraining Loss: 14.1243\n",
      "Epoch [84/500], Pretraining Loss: 14.1218\n",
      "Epoch [85/500], Pretraining Loss: 13.9733\n",
      "Epoch [86/500], Pretraining Loss: 13.9046\n",
      "Epoch [87/500], Pretraining Loss: 13.8937\n",
      "Epoch [88/500], Pretraining Loss: 13.8010\n",
      "Epoch [89/500], Pretraining Loss: 13.7928\n",
      "Epoch [90/500], Pretraining Loss: 13.5517\n",
      "Epoch [91/500], Pretraining Loss: 13.5538\n",
      "Epoch [92/500], Pretraining Loss: 13.4439\n",
      "Epoch [93/500], Pretraining Loss: 13.4887\n",
      "Epoch [94/500], Pretraining Loss: 13.2906\n",
      "Epoch [95/500], Pretraining Loss: 13.1550\n",
      "Epoch [96/500], Pretraining Loss: 13.1040\n",
      "Epoch [97/500], Pretraining Loss: 13.1567\n",
      "Epoch [98/500], Pretraining Loss: 13.0829\n",
      "Epoch [99/500], Pretraining Loss: 12.9888\n",
      "Epoch [100/500], Pretraining Loss: 12.9483\n",
      "Epoch [101/500], Pretraining Loss: 12.8646\n",
      "Epoch [102/500], Pretraining Loss: 12.8271\n",
      "Epoch [103/500], Pretraining Loss: 12.7785\n",
      "Epoch [104/500], Pretraining Loss: 12.7397\n",
      "Epoch [105/500], Pretraining Loss: 12.5737\n",
      "Epoch [106/500], Pretraining Loss: 12.4101\n",
      "Epoch [107/500], Pretraining Loss: 12.6304\n",
      "Epoch [108/500], Pretraining Loss: 12.3035\n",
      "Epoch [109/500], Pretraining Loss: 12.2235\n",
      "Epoch [110/500], Pretraining Loss: 12.2010\n",
      "Epoch [111/500], Pretraining Loss: 12.0973\n",
      "Epoch [112/500], Pretraining Loss: 12.0658\n",
      "Epoch [113/500], Pretraining Loss: 12.0901\n",
      "Epoch [114/500], Pretraining Loss: 11.9562\n",
      "Epoch [115/500], Pretraining Loss: 11.8967\n",
      "Epoch [116/500], Pretraining Loss: 11.9377\n",
      "Epoch [117/500], Pretraining Loss: 11.6947\n",
      "Epoch [118/500], Pretraining Loss: 11.8057\n",
      "Epoch [119/500], Pretraining Loss: 11.7131\n",
      "Epoch [120/500], Pretraining Loss: 11.6261\n",
      "Epoch [121/500], Pretraining Loss: 11.4844\n",
      "Epoch [122/500], Pretraining Loss: 11.4634\n",
      "Epoch [123/500], Pretraining Loss: 11.3448\n",
      "Epoch [124/500], Pretraining Loss: 11.4576\n",
      "Epoch [125/500], Pretraining Loss: 11.2199\n",
      "Epoch [126/500], Pretraining Loss: 11.2354\n",
      "Epoch [127/500], Pretraining Loss: 11.1775\n",
      "Epoch [128/500], Pretraining Loss: 11.1299\n",
      "Epoch [129/500], Pretraining Loss: 11.0091\n",
      "Epoch [130/500], Pretraining Loss: 11.0756\n",
      "Epoch [131/500], Pretraining Loss: 10.9444\n",
      "Epoch [132/500], Pretraining Loss: 10.7979\n",
      "Epoch [133/500], Pretraining Loss: 10.7564\n",
      "Epoch [134/500], Pretraining Loss: 10.6743\n",
      "Epoch [135/500], Pretraining Loss: 10.6840\n",
      "Epoch [136/500], Pretraining Loss: 10.6270\n",
      "Epoch [137/500], Pretraining Loss: 10.3943\n",
      "Epoch [138/500], Pretraining Loss: 10.5980\n",
      "Epoch [139/500], Pretraining Loss: 10.4051\n",
      "Epoch [140/500], Pretraining Loss: 10.3532\n",
      "Epoch [141/500], Pretraining Loss: 10.3923\n",
      "Epoch [142/500], Pretraining Loss: 10.3348\n",
      "Epoch [143/500], Pretraining Loss: 10.1064\n",
      "Epoch [144/500], Pretraining Loss: 10.0161\n",
      "Epoch [145/500], Pretraining Loss: 10.0366\n",
      "Epoch [146/500], Pretraining Loss: 9.9854\n",
      "Epoch [147/500], Pretraining Loss: 10.0012\n",
      "Epoch [148/500], Pretraining Loss: 9.9179\n",
      "Epoch [149/500], Pretraining Loss: 9.8942\n",
      "Epoch [150/500], Pretraining Loss: 9.7749\n",
      "Epoch [151/500], Pretraining Loss: 9.7700\n",
      "Epoch [152/500], Pretraining Loss: 9.7597\n",
      "Epoch [153/500], Pretraining Loss: 9.6292\n",
      "Epoch [154/500], Pretraining Loss: 9.4581\n",
      "Epoch [155/500], Pretraining Loss: 9.5052\n",
      "Epoch [156/500], Pretraining Loss: 9.4706\n",
      "Epoch [157/500], Pretraining Loss: 9.4084\n",
      "Epoch [158/500], Pretraining Loss: 9.2555\n",
      "Epoch [159/500], Pretraining Loss: 9.2875\n",
      "Epoch [160/500], Pretraining Loss: 9.2682\n",
      "Epoch [161/500], Pretraining Loss: 9.1257\n",
      "Epoch [162/500], Pretraining Loss: 9.1329\n",
      "Epoch [163/500], Pretraining Loss: 8.9837\n",
      "Epoch [164/500], Pretraining Loss: 8.9582\n",
      "Epoch [165/500], Pretraining Loss: 8.9785\n",
      "Epoch [166/500], Pretraining Loss: 8.9617\n",
      "Epoch [167/500], Pretraining Loss: 8.8345\n",
      "Epoch [168/500], Pretraining Loss: 8.8538\n",
      "Epoch [169/500], Pretraining Loss: 8.7707\n",
      "Epoch [170/500], Pretraining Loss: 8.6261\n",
      "Epoch [171/500], Pretraining Loss: 8.6105\n",
      "Epoch [172/500], Pretraining Loss: 8.6077\n",
      "Epoch [173/500], Pretraining Loss: 8.4277\n",
      "Epoch [174/500], Pretraining Loss: 8.5334\n",
      "Epoch [175/500], Pretraining Loss: 8.5149\n",
      "Epoch [176/500], Pretraining Loss: 8.3532\n",
      "Epoch [177/500], Pretraining Loss: 8.3548\n",
      "Epoch [178/500], Pretraining Loss: 8.3110\n",
      "Epoch [179/500], Pretraining Loss: 8.2720\n",
      "Epoch [180/500], Pretraining Loss: 8.1109\n",
      "Epoch [181/500], Pretraining Loss: 8.0789\n",
      "Epoch [182/500], Pretraining Loss: 8.0774\n",
      "Epoch [183/500], Pretraining Loss: 8.0117\n",
      "Epoch [184/500], Pretraining Loss: 7.9582\n",
      "Epoch [185/500], Pretraining Loss: 7.9283\n",
      "Epoch [186/500], Pretraining Loss: 7.8564\n",
      "Epoch [187/500], Pretraining Loss: 7.8585\n",
      "Epoch [188/500], Pretraining Loss: 7.7599\n",
      "Epoch [189/500], Pretraining Loss: 7.6854\n",
      "Epoch [190/500], Pretraining Loss: 7.6610\n",
      "Epoch [191/500], Pretraining Loss: 7.6588\n",
      "Epoch [192/500], Pretraining Loss: 7.5648\n",
      "Epoch [193/500], Pretraining Loss: 7.5132\n",
      "Epoch [194/500], Pretraining Loss: 7.4709\n",
      "Epoch [195/500], Pretraining Loss: 7.3808\n",
      "Epoch [196/500], Pretraining Loss: 7.4055\n",
      "Epoch [197/500], Pretraining Loss: 7.3114\n",
      "Epoch [198/500], Pretraining Loss: 7.2638\n",
      "Epoch [199/500], Pretraining Loss: 7.2089\n",
      "Epoch [200/500], Pretraining Loss: 7.1673\n",
      "Epoch [201/500], Pretraining Loss: 7.1332\n",
      "Epoch [202/500], Pretraining Loss: 7.0398\n",
      "Epoch [203/500], Pretraining Loss: 7.0133\n",
      "Epoch [204/500], Pretraining Loss: 6.9989\n",
      "Epoch [205/500], Pretraining Loss: 6.9242\n",
      "Epoch [206/500], Pretraining Loss: 6.8666\n",
      "Epoch [207/500], Pretraining Loss: 6.8747\n",
      "Epoch [208/500], Pretraining Loss: 6.8179\n",
      "Epoch [209/500], Pretraining Loss: 6.7663\n",
      "Epoch [210/500], Pretraining Loss: 6.7093\n",
      "Epoch [211/500], Pretraining Loss: 6.7220\n",
      "Epoch [212/500], Pretraining Loss: 6.6198\n",
      "Epoch [213/500], Pretraining Loss: 6.6270\n",
      "Epoch [214/500], Pretraining Loss: 6.5590\n",
      "Epoch [215/500], Pretraining Loss: 6.5010\n",
      "Epoch [216/500], Pretraining Loss: 6.4644\n",
      "Epoch [217/500], Pretraining Loss: 6.4769\n",
      "Epoch [218/500], Pretraining Loss: 6.4103\n",
      "Epoch [219/500], Pretraining Loss: 6.3359\n",
      "Epoch [220/500], Pretraining Loss: 6.2805\n",
      "Epoch [221/500], Pretraining Loss: 6.2417\n",
      "Epoch [222/500], Pretraining Loss: 6.2228\n",
      "Epoch [223/500], Pretraining Loss: 6.2530\n",
      "Epoch [224/500], Pretraining Loss: 6.1765\n",
      "Epoch [225/500], Pretraining Loss: 6.1675\n",
      "Epoch [226/500], Pretraining Loss: 6.0932\n",
      "Epoch [227/500], Pretraining Loss: 6.0668\n",
      "Epoch [228/500], Pretraining Loss: 6.0295\n",
      "Epoch [229/500], Pretraining Loss: 5.9688\n",
      "Epoch [230/500], Pretraining Loss: 5.8461\n",
      "Epoch [231/500], Pretraining Loss: 5.8345\n",
      "Epoch [232/500], Pretraining Loss: 5.8592\n",
      "Epoch [233/500], Pretraining Loss: 5.6929\n",
      "Epoch [234/500], Pretraining Loss: 5.7729\n",
      "Epoch [235/500], Pretraining Loss: 5.7033\n",
      "Epoch [236/500], Pretraining Loss: 5.6828\n",
      "Epoch [237/500], Pretraining Loss: 5.6468\n",
      "Epoch [238/500], Pretraining Loss: 5.6108\n",
      "Epoch [239/500], Pretraining Loss: 5.5591\n",
      "Epoch [240/500], Pretraining Loss: 5.5304\n",
      "Epoch [241/500], Pretraining Loss: 5.4763\n",
      "Epoch [242/500], Pretraining Loss: 5.3761\n",
      "Epoch [243/500], Pretraining Loss: 5.4286\n",
      "Epoch [244/500], Pretraining Loss: 5.3275\n",
      "Epoch [245/500], Pretraining Loss: 5.3433\n",
      "Epoch [246/500], Pretraining Loss: 5.3062\n",
      "Epoch [247/500], Pretraining Loss: 5.2645\n",
      "Epoch [248/500], Pretraining Loss: 5.2245\n",
      "Epoch [249/500], Pretraining Loss: 5.2232\n",
      "Epoch [250/500], Pretraining Loss: 5.1992\n",
      "Epoch [251/500], Pretraining Loss: 5.1419\n",
      "Epoch [252/500], Pretraining Loss: 5.1209\n",
      "Epoch [253/500], Pretraining Loss: 5.0501\n",
      "Epoch [254/500], Pretraining Loss: 5.0111\n",
      "Epoch [255/500], Pretraining Loss: 4.9909\n",
      "Epoch [256/500], Pretraining Loss: 4.9196\n",
      "Epoch [257/500], Pretraining Loss: 4.9475\n",
      "Epoch [258/500], Pretraining Loss: 4.9211\n",
      "Epoch [259/500], Pretraining Loss: 4.8279\n",
      "Epoch [260/500], Pretraining Loss: 4.8198\n",
      "Epoch [261/500], Pretraining Loss: 4.7538\n",
      "Epoch [262/500], Pretraining Loss: 4.7792\n",
      "Epoch [263/500], Pretraining Loss: 4.7349\n",
      "Epoch [264/500], Pretraining Loss: 4.7214\n",
      "Epoch [265/500], Pretraining Loss: 4.6699\n",
      "Epoch [266/500], Pretraining Loss: 4.6061\n",
      "Epoch [267/500], Pretraining Loss: 4.5377\n",
      "Epoch [268/500], Pretraining Loss: 4.5517\n",
      "Epoch [269/500], Pretraining Loss: 4.5139\n",
      "Epoch [270/500], Pretraining Loss: 4.4636\n",
      "Epoch [271/500], Pretraining Loss: 4.4963\n",
      "Epoch [272/500], Pretraining Loss: 4.4837\n",
      "Epoch [273/500], Pretraining Loss: 4.3770\n",
      "Epoch [274/500], Pretraining Loss: 4.4046\n",
      "Epoch [275/500], Pretraining Loss: 4.3412\n",
      "Epoch [276/500], Pretraining Loss: 4.3550\n",
      "Epoch [277/500], Pretraining Loss: 4.2569\n",
      "Epoch [278/500], Pretraining Loss: 4.2578\n",
      "Epoch [279/500], Pretraining Loss: 4.1941\n",
      "Epoch [280/500], Pretraining Loss: 4.2165\n",
      "Epoch [281/500], Pretraining Loss: 4.1656\n",
      "Epoch [282/500], Pretraining Loss: 4.1280\n",
      "Epoch [283/500], Pretraining Loss: 4.1254\n",
      "Epoch [284/500], Pretraining Loss: 4.0591\n",
      "Epoch [285/500], Pretraining Loss: 4.0300\n",
      "Epoch [286/500], Pretraining Loss: 4.0023\n",
      "Epoch [287/500], Pretraining Loss: 4.0464\n",
      "Epoch [288/500], Pretraining Loss: 3.9857\n",
      "Epoch [289/500], Pretraining Loss: 3.9020\n",
      "Epoch [290/500], Pretraining Loss: 3.8931\n",
      "Epoch [291/500], Pretraining Loss: 3.8631\n",
      "Epoch [292/500], Pretraining Loss: 3.8405\n",
      "Epoch [293/500], Pretraining Loss: 3.8123\n",
      "Epoch [294/500], Pretraining Loss: 3.7957\n",
      "Epoch [295/500], Pretraining Loss: 3.7577\n",
      "Epoch [296/500], Pretraining Loss: 3.7192\n",
      "Epoch [297/500], Pretraining Loss: 3.7323\n",
      "Epoch [298/500], Pretraining Loss: 3.6651\n",
      "Epoch [299/500], Pretraining Loss: 3.6583\n",
      "Epoch [300/500], Pretraining Loss: 3.6126\n",
      "Epoch [301/500], Pretraining Loss: 3.5813\n",
      "Epoch [302/500], Pretraining Loss: 3.5848\n",
      "Epoch [303/500], Pretraining Loss: 3.5513\n",
      "Epoch [304/500], Pretraining Loss: 3.5620\n",
      "Epoch [305/500], Pretraining Loss: 3.4997\n",
      "Epoch [306/500], Pretraining Loss: 3.5209\n",
      "Epoch [307/500], Pretraining Loss: 3.4670\n",
      "Epoch [308/500], Pretraining Loss: 3.4203\n",
      "Epoch [309/500], Pretraining Loss: 3.3898\n",
      "Epoch [310/500], Pretraining Loss: 3.3842\n",
      "Epoch [311/500], Pretraining Loss: 3.3449\n",
      "Epoch [312/500], Pretraining Loss: 3.3378\n",
      "Epoch [313/500], Pretraining Loss: 3.3016\n",
      "Epoch [314/500], Pretraining Loss: 3.2712\n",
      "Epoch [315/500], Pretraining Loss: 3.2767\n",
      "Epoch [316/500], Pretraining Loss: 3.2547\n",
      "Epoch [317/500], Pretraining Loss: 3.2216\n",
      "Epoch [318/500], Pretraining Loss: 3.1789\n",
      "Epoch [319/500], Pretraining Loss: 3.1543\n",
      "Epoch [320/500], Pretraining Loss: 3.1550\n",
      "Epoch [321/500], Pretraining Loss: 3.1289\n",
      "Epoch [322/500], Pretraining Loss: 3.0910\n",
      "Epoch [323/500], Pretraining Loss: 3.0611\n",
      "Epoch [324/500], Pretraining Loss: 3.0592\n",
      "Epoch [325/500], Pretraining Loss: 3.0058\n",
      "Epoch [326/500], Pretraining Loss: 3.0176\n",
      "Epoch [327/500], Pretraining Loss: 3.0065\n",
      "Epoch [328/500], Pretraining Loss: 2.9384\n",
      "Epoch [329/500], Pretraining Loss: 2.9366\n",
      "Epoch [330/500], Pretraining Loss: 2.9184\n",
      "Epoch [331/500], Pretraining Loss: 2.9184\n",
      "Epoch [332/500], Pretraining Loss: 2.8364\n",
      "Epoch [333/500], Pretraining Loss: 2.8179\n",
      "Epoch [334/500], Pretraining Loss: 2.8152\n",
      "Epoch [335/500], Pretraining Loss: 2.8001\n",
      "Epoch [336/500], Pretraining Loss: 2.8078\n",
      "Epoch [337/500], Pretraining Loss: 2.7385\n",
      "Epoch [338/500], Pretraining Loss: 2.7636\n",
      "Epoch [339/500], Pretraining Loss: 2.7445\n",
      "Epoch [340/500], Pretraining Loss: 2.6949\n",
      "Epoch [341/500], Pretraining Loss: 2.7194\n",
      "Epoch [342/500], Pretraining Loss: 2.6564\n",
      "Epoch [343/500], Pretraining Loss: 2.6561\n",
      "Epoch [344/500], Pretraining Loss: 2.6448\n",
      "Epoch [345/500], Pretraining Loss: 2.5958\n",
      "Epoch [346/500], Pretraining Loss: 2.5901\n",
      "Epoch [347/500], Pretraining Loss: 2.5903\n",
      "Epoch [348/500], Pretraining Loss: 2.5673\n",
      "Epoch [349/500], Pretraining Loss: 2.5422\n",
      "Epoch [350/500], Pretraining Loss: 2.5080\n",
      "Epoch [351/500], Pretraining Loss: 2.5124\n",
      "Epoch [352/500], Pretraining Loss: 2.4740\n",
      "Epoch [353/500], Pretraining Loss: 2.4570\n",
      "Epoch [354/500], Pretraining Loss: 2.4447\n",
      "Epoch [355/500], Pretraining Loss: 2.4317\n",
      "Epoch [356/500], Pretraining Loss: 2.4113\n",
      "Epoch [357/500], Pretraining Loss: 2.4135\n",
      "Epoch [358/500], Pretraining Loss: 2.3715\n",
      "Epoch [359/500], Pretraining Loss: 2.3628\n",
      "Epoch [360/500], Pretraining Loss: 2.3414\n",
      "Epoch [361/500], Pretraining Loss: 2.3137\n",
      "Epoch [362/500], Pretraining Loss: 2.2839\n",
      "Epoch [363/500], Pretraining Loss: 2.2971\n",
      "Epoch [364/500], Pretraining Loss: 2.2350\n",
      "Epoch [365/500], Pretraining Loss: 2.2595\n",
      "Epoch [366/500], Pretraining Loss: 2.2303\n",
      "Epoch [367/500], Pretraining Loss: 2.2082\n",
      "Epoch [368/500], Pretraining Loss: 2.1933\n",
      "Epoch [369/500], Pretraining Loss: 2.1829\n",
      "Epoch [370/500], Pretraining Loss: 2.1575\n",
      "Epoch [371/500], Pretraining Loss: 2.1329\n",
      "Epoch [372/500], Pretraining Loss: 2.1543\n",
      "Epoch [373/500], Pretraining Loss: 2.0901\n",
      "Epoch [374/500], Pretraining Loss: 2.0934\n",
      "Epoch [375/500], Pretraining Loss: 2.0749\n",
      "Epoch [376/500], Pretraining Loss: 2.0736\n",
      "Epoch [377/500], Pretraining Loss: 2.0436\n",
      "Epoch [378/500], Pretraining Loss: 2.0458\n",
      "Epoch [379/500], Pretraining Loss: 2.0194\n",
      "Epoch [380/500], Pretraining Loss: 2.0259\n",
      "Epoch [381/500], Pretraining Loss: 1.9932\n",
      "Epoch [382/500], Pretraining Loss: 1.9855\n",
      "Epoch [383/500], Pretraining Loss: 1.9603\n",
      "Epoch [384/500], Pretraining Loss: 1.9521\n",
      "Epoch [385/500], Pretraining Loss: 1.9373\n",
      "Epoch [386/500], Pretraining Loss: 1.9239\n",
      "Epoch [387/500], Pretraining Loss: 1.9015\n",
      "Epoch [388/500], Pretraining Loss: 1.8904\n",
      "Epoch [389/500], Pretraining Loss: 1.8784\n",
      "Epoch [390/500], Pretraining Loss: 1.8617\n",
      "Epoch [391/500], Pretraining Loss: 1.8542\n",
      "Epoch [392/500], Pretraining Loss: 1.8447\n",
      "Epoch [393/500], Pretraining Loss: 1.8196\n",
      "Epoch [394/500], Pretraining Loss: 1.8019\n",
      "Epoch [395/500], Pretraining Loss: 1.7838\n",
      "Epoch [396/500], Pretraining Loss: 1.7858\n",
      "Epoch [397/500], Pretraining Loss: 1.7601\n",
      "Epoch [398/500], Pretraining Loss: 1.7496\n",
      "Epoch [399/500], Pretraining Loss: 1.7403\n",
      "Epoch [400/500], Pretraining Loss: 1.7141\n",
      "Epoch [401/500], Pretraining Loss: 1.7298\n",
      "Epoch [402/500], Pretraining Loss: 1.7115\n",
      "Epoch [403/500], Pretraining Loss: 1.6677\n",
      "Epoch [404/500], Pretraining Loss: 1.6610\n",
      "Epoch [405/500], Pretraining Loss: 1.6504\n",
      "Epoch [406/500], Pretraining Loss: 1.6377\n",
      "Epoch [407/500], Pretraining Loss: 1.6499\n",
      "Epoch [408/500], Pretraining Loss: 1.6111\n",
      "Epoch [409/500], Pretraining Loss: 1.6133\n",
      "Epoch [410/500], Pretraining Loss: 1.6106\n",
      "Epoch [411/500], Pretraining Loss: 1.5765\n",
      "Epoch [412/500], Pretraining Loss: 1.5759\n",
      "Epoch [413/500], Pretraining Loss: 1.5518\n",
      "Epoch [414/500], Pretraining Loss: 1.5448\n",
      "Epoch [415/500], Pretraining Loss: 1.5233\n",
      "Epoch [416/500], Pretraining Loss: 1.5261\n",
      "Epoch [417/500], Pretraining Loss: 1.4922\n",
      "Epoch [418/500], Pretraining Loss: 1.4816\n",
      "Epoch [419/500], Pretraining Loss: 1.4831\n",
      "Epoch [420/500], Pretraining Loss: 1.4795\n",
      "Epoch [421/500], Pretraining Loss: 1.4661\n",
      "Epoch [422/500], Pretraining Loss: 1.4518\n",
      "Epoch [423/500], Pretraining Loss: 1.4254\n",
      "Epoch [424/500], Pretraining Loss: 1.4287\n",
      "Epoch [425/500], Pretraining Loss: 1.4115\n",
      "Epoch [426/500], Pretraining Loss: 1.4149\n",
      "Epoch [427/500], Pretraining Loss: 1.3962\n",
      "Epoch [428/500], Pretraining Loss: 1.3914\n",
      "Epoch [429/500], Pretraining Loss: 1.3720\n",
      "Epoch [430/500], Pretraining Loss: 1.3452\n",
      "Epoch [431/500], Pretraining Loss: 1.3522\n",
      "Epoch [432/500], Pretraining Loss: 1.3398\n",
      "Epoch [433/500], Pretraining Loss: 1.3214\n",
      "Epoch [434/500], Pretraining Loss: 1.3111\n",
      "Epoch [435/500], Pretraining Loss: 1.3114\n",
      "Epoch [436/500], Pretraining Loss: 1.2797\n",
      "Epoch [437/500], Pretraining Loss: 1.2865\n",
      "Epoch [438/500], Pretraining Loss: 1.2895\n",
      "Epoch [439/500], Pretraining Loss: 1.2645\n",
      "Epoch [440/500], Pretraining Loss: 1.2645\n",
      "Epoch [441/500], Pretraining Loss: 1.2492\n",
      "Epoch [442/500], Pretraining Loss: 1.2403\n",
      "Epoch [443/500], Pretraining Loss: 1.2328\n",
      "Epoch [444/500], Pretraining Loss: 1.2035\n",
      "Epoch [445/500], Pretraining Loss: 1.2108\n",
      "Epoch [446/500], Pretraining Loss: 1.2035\n",
      "Epoch [447/500], Pretraining Loss: 1.1907\n",
      "Epoch [448/500], Pretraining Loss: 1.1938\n",
      "Epoch [449/500], Pretraining Loss: 1.1582\n",
      "Epoch [450/500], Pretraining Loss: 1.1486\n",
      "Epoch [451/500], Pretraining Loss: 1.1466\n",
      "Epoch [452/500], Pretraining Loss: 1.1497\n",
      "Epoch [453/500], Pretraining Loss: 1.1451\n",
      "Epoch [454/500], Pretraining Loss: 1.1306\n",
      "Epoch [455/500], Pretraining Loss: 1.1224\n",
      "Epoch [456/500], Pretraining Loss: 1.1018\n",
      "Epoch [457/500], Pretraining Loss: 1.0953\n",
      "Epoch [458/500], Pretraining Loss: 1.0812\n",
      "Epoch [459/500], Pretraining Loss: 1.0800\n",
      "Epoch [460/500], Pretraining Loss: 1.0725\n",
      "Epoch [461/500], Pretraining Loss: 1.0603\n",
      "Epoch [462/500], Pretraining Loss: 1.0549\n",
      "Epoch [463/500], Pretraining Loss: 1.0440\n",
      "Epoch [464/500], Pretraining Loss: 1.0404\n",
      "Epoch [465/500], Pretraining Loss: 1.0294\n",
      "Epoch [466/500], Pretraining Loss: 1.0259\n",
      "Epoch [467/500], Pretraining Loss: 1.0237\n",
      "Epoch [468/500], Pretraining Loss: 1.0047\n",
      "Epoch [469/500], Pretraining Loss: 0.9948\n",
      "Epoch [470/500], Pretraining Loss: 1.0021\n",
      "Epoch [471/500], Pretraining Loss: 0.9804\n",
      "Epoch [472/500], Pretraining Loss: 0.9800\n",
      "Epoch [473/500], Pretraining Loss: 0.9692\n",
      "Epoch [474/500], Pretraining Loss: 0.9599\n",
      "Epoch [475/500], Pretraining Loss: 0.9516\n",
      "Epoch [476/500], Pretraining Loss: 0.9494\n",
      "Epoch [477/500], Pretraining Loss: 0.9345\n",
      "Epoch [478/500], Pretraining Loss: 0.9303\n",
      "Epoch [479/500], Pretraining Loss: 0.9155\n",
      "Epoch [480/500], Pretraining Loss: 0.9193\n",
      "Epoch [481/500], Pretraining Loss: 0.9081\n",
      "Epoch [482/500], Pretraining Loss: 0.9007\n",
      "Epoch [483/500], Pretraining Loss: 0.8960\n",
      "Epoch [484/500], Pretraining Loss: 0.8896\n",
      "Epoch [485/500], Pretraining Loss: 0.8769\n",
      "Epoch [486/500], Pretraining Loss: 0.8735\n",
      "Epoch [487/500], Pretraining Loss: 0.8638\n",
      "Epoch [488/500], Pretraining Loss: 0.8607\n",
      "Epoch [489/500], Pretraining Loss: 0.8514\n",
      "Epoch [490/500], Pretraining Loss: 0.8376\n",
      "Epoch [491/500], Pretraining Loss: 0.8435\n",
      "Epoch [492/500], Pretraining Loss: 0.8287\n",
      "Epoch [493/500], Pretraining Loss: 0.8288\n",
      "Epoch [494/500], Pretraining Loss: 0.8242\n",
      "Epoch [495/500], Pretraining Loss: 0.8128\n",
      "Epoch [496/500], Pretraining Loss: 0.8018\n",
      "Epoch [497/500], Pretraining Loss: 0.7977\n",
      "Epoch [498/500], Pretraining Loss: 0.7936\n",
      "Epoch [499/500], Pretraining Loss: 0.7879\n",
      "Epoch [500/500], Pretraining Loss: 0.7725\n",
      "Pretrained model saved to embeddings/bitcoin_alpha_ca_sdgnn_pretrained.pth\n",
      "Pretrained model saved to embeddings/bitcoin_alpha_ca_sdgnn_pretrained.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\abhin\\Comding\\ML\\Capstone\\casdgnn\\data_utils.py:138: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:656.)\n",
      "  signed_adj_matrix = torch.sparse.FloatTensor(indices, values, (num_nodes, num_nodes))\n",
      "\n",
      "Pretraining Epochs:   0%|          | 0/500 [00:00<?, ?it/s]\n",
      "Pretraining Epochs:   0%|          | 1/500 [00:03<26:10,  3.15s/it]\n",
      "Pretraining Epochs:   0%|          | 2/500 [00:03<11:28,  1.38s/it]\n",
      "Pretraining Epochs:   1%|          | 3/500 [00:03<06:44,  1.23it/s]\n",
      "Pretraining Epochs:   1%|          | 4/500 [00:03<04:32,  1.82it/s]\n",
      "Pretraining Epochs:   1%|          | 5/500 [00:03<03:23,  2.43it/s]\n",
      "Pretraining Epochs:   1%|          | 6/500 [00:03<02:37,  3.13it/s]\n",
      "Pretraining Epochs:   1%|▏         | 7/500 [00:04<02:07,  3.88it/s]\n",
      "Pretraining Epochs:   2%|▏         | 8/500 [00:04<01:50,  4.47it/s]\n",
      "Pretraining Epochs:   2%|▏         | 9/500 [00:04<01:36,  5.10it/s]\n",
      "Pretraining Epochs:   2%|▏         | 10/500 [00:04<01:27,  5.61it/s]\n",
      "Pretraining Epochs:   2%|▏         | 11/500 [00:04<01:24,  5.81it/s]\n",
      "Pretraining Epochs:   2%|▏         | 12/500 [00:04<01:24,  5.80it/s]\n",
      "Pretraining Epochs:   3%|▎         | 13/500 [00:04<01:20,  6.09it/s]\n",
      "Pretraining Epochs:   3%|▎         | 14/500 [00:05<01:15,  6.44it/s]\n",
      "Pretraining Epochs:   3%|▎         | 15/500 [00:05<01:11,  6.76it/s]\n",
      "Pretraining Epochs:   3%|▎         | 16/500 [00:05<01:09,  6.99it/s]\n",
      "Pretraining Epochs:   3%|▎         | 17/500 [00:05<01:07,  7.11it/s]\n",
      "Pretraining Epochs:   4%|▎         | 18/500 [00:05<01:08,  7.03it/s]\n",
      "Pretraining Epochs:   4%|▍         | 19/500 [00:05<01:06,  7.19it/s]\n",
      "Pretraining Epochs:   4%|▍         | 20/500 [00:05<01:08,  7.01it/s]\n",
      "Pretraining Epochs:   4%|▍         | 21/500 [00:06<01:10,  6.83it/s]\n",
      "Pretraining Epochs:   4%|▍         | 22/500 [00:06<01:10,  6.81it/s]\n",
      "Pretraining Epochs:   5%|▍         | 23/500 [00:06<01:10,  6.76it/s]\n",
      "Pretraining Epochs:   5%|▍         | 24/500 [00:06<01:08,  6.91it/s]\n",
      "Pretraining Epochs:   5%|▌         | 25/500 [00:06<01:08,  6.93it/s]\n",
      "Pretraining Epochs:   5%|▌         | 26/500 [00:06<01:07,  7.06it/s]\n",
      "Pretraining Epochs:   5%|▌         | 27/500 [00:06<01:07,  6.99it/s]\n",
      "Pretraining Epochs:   6%|▌         | 28/500 [00:07<01:05,  7.25it/s]\n",
      "Pretraining Epochs:   6%|▌         | 29/500 [00:07<01:04,  7.28it/s]\n",
      "Pretraining Epochs:   6%|▌         | 30/500 [00:07<01:05,  7.16it/s]\n",
      "Pretraining Epochs:   6%|▌         | 31/500 [00:07<01:04,  7.25it/s]\n",
      "Pretraining Epochs:   6%|▋         | 32/500 [00:07<01:08,  6.84it/s]\n",
      "Pretraining Epochs:   7%|▋         | 33/500 [00:07<01:07,  6.88it/s]\n",
      "Pretraining Epochs:   7%|▋         | 34/500 [00:07<01:09,  6.72it/s]\n",
      "Pretraining Epochs:   7%|▋         | 35/500 [00:08<01:09,  6.68it/s]\n",
      "Pretraining Epochs:   7%|▋         | 36/500 [00:08<01:06,  6.99it/s]\n",
      "Pretraining Epochs:   7%|▋         | 37/500 [00:08<01:07,  6.88it/s]\n",
      "Pretraining Epochs:   8%|▊         | 38/500 [00:08<01:08,  6.74it/s]\n",
      "Pretraining Epochs:   8%|▊         | 39/500 [00:08<01:08,  6.77it/s]\n",
      "Pretraining Epochs:   8%|▊         | 40/500 [00:08<01:06,  6.94it/s]\n",
      "Pretraining Epochs:   8%|▊         | 41/500 [00:08<01:06,  6.92it/s]\n",
      "Pretraining Epochs:   8%|▊         | 42/500 [00:09<01:04,  7.11it/s]\n",
      "Pretraining Epochs:   9%|▊         | 43/500 [00:09<01:05,  6.93it/s]\n",
      "Pretraining Epochs:   9%|▉         | 44/500 [00:09<01:06,  6.84it/s]\n",
      "Pretraining Epochs:   9%|▉         | 45/500 [00:09<01:05,  6.99it/s]\n",
      "Pretraining Epochs:   9%|▉         | 46/500 [00:09<01:07,  6.68it/s]\n",
      "Pretraining Epochs:   9%|▉         | 47/500 [00:09<01:05,  6.87it/s]\n",
      "Pretraining Epochs:  10%|▉         | 48/500 [00:09<01:05,  6.94it/s]\n",
      "Pretraining Epochs:  10%|▉         | 49/500 [00:10<01:05,  6.91it/s]\n",
      "Pretraining Epochs:  10%|█         | 50/500 [00:10<01:04,  6.97it/s]\n",
      "Pretraining Epochs:  10%|█         | 51/500 [00:10<01:05,  6.83it/s]\n",
      "Pretraining Epochs:  10%|█         | 52/500 [00:10<01:04,  6.96it/s]\n",
      "Pretraining Epochs:  11%|█         | 53/500 [00:10<01:04,  6.98it/s]\n",
      "Pretraining Epochs:  11%|█         | 54/500 [00:10<01:03,  7.00it/s]\n",
      "Pretraining Epochs:  11%|█         | 55/500 [00:10<01:04,  6.93it/s]\n",
      "Pretraining Epochs:  11%|█         | 56/500 [00:11<01:07,  6.59it/s]\n",
      "Pretraining Epochs:  11%|█▏        | 57/500 [00:11<01:07,  6.59it/s]\n",
      "Pretraining Epochs:  12%|█▏        | 58/500 [00:11<01:08,  6.44it/s]\n",
      "Pretraining Epochs:  12%|█▏        | 59/500 [00:11<01:06,  6.60it/s]\n",
      "Pretraining Epochs:  12%|█▏        | 60/500 [00:11<01:04,  6.83it/s]\n",
      "Pretraining Epochs:  12%|█▏        | 61/500 [00:11<01:04,  6.76it/s]\n",
      "Pretraining Epochs:  12%|█▏        | 62/500 [00:11<01:04,  6.80it/s]\n",
      "Pretraining Epochs:  13%|█▎        | 63/500 [00:12<01:04,  6.81it/s]\n",
      "Pretraining Epochs:  13%|█▎        | 64/500 [00:12<01:02,  6.92it/s]\n",
      "Pretraining Epochs:  13%|█▎        | 65/500 [00:12<01:03,  6.83it/s]\n",
      "Pretraining Epochs:  13%|█▎        | 66/500 [00:12<01:03,  6.79it/s]\n",
      "Pretraining Epochs:  13%|█▎        | 67/500 [00:12<01:06,  6.51it/s]\n",
      "Pretraining Epochs:  14%|█▎        | 68/500 [00:12<01:04,  6.73it/s]\n",
      "Pretraining Epochs:  14%|█▍        | 69/500 [00:13<01:04,  6.71it/s]\n",
      "Pretraining Epochs:  14%|█▍        | 70/500 [00:13<01:11,  6.00it/s]\n",
      "Pretraining Epochs:  14%|█▍        | 71/500 [00:13<01:08,  6.30it/s]\n",
      "Pretraining Epochs:  14%|█▍        | 72/500 [00:13<01:05,  6.51it/s]\n",
      "Pretraining Epochs:  15%|█▍        | 73/500 [00:13<01:03,  6.77it/s]\n",
      "Pretraining Epochs:  15%|█▍        | 74/500 [00:13<01:02,  6.84it/s]\n",
      "Pretraining Epochs:  15%|█▌        | 75/500 [00:13<01:03,  6.68it/s]\n",
      "Pretraining Epochs:  15%|█▌        | 76/500 [00:14<01:06,  6.39it/s]\n",
      "Pretraining Epochs:  15%|█▌        | 77/500 [00:14<01:12,  5.80it/s]\n",
      "Pretraining Epochs:  16%|█▌        | 78/500 [00:14<01:08,  6.13it/s]\n",
      "Pretraining Epochs:  16%|█▌        | 79/500 [00:14<01:04,  6.50it/s]\n",
      "Pretraining Epochs:  16%|█▌        | 80/500 [00:14<01:01,  6.82it/s]\n",
      "Pretraining Epochs:  16%|█▌        | 81/500 [00:14<01:01,  6.82it/s]\n",
      "Pretraining Epochs:  16%|█▋        | 82/500 [00:15<01:00,  6.93it/s]\n",
      "Pretraining Epochs:  17%|█▋        | 83/500 [00:15<00:59,  6.96it/s]\n",
      "Pretraining Epochs:  17%|█▋        | 84/500 [00:15<01:02,  6.64it/s]\n",
      "Pretraining Epochs:  17%|█▋        | 85/500 [00:15<01:05,  6.34it/s]\n",
      "Pretraining Epochs:  17%|█▋        | 86/500 [00:15<01:01,  6.69it/s]\n",
      "Pretraining Epochs:  17%|█▋        | 87/500 [00:15<00:59,  6.93it/s]\n",
      "Pretraining Epochs:  18%|█▊        | 88/500 [00:15<00:58,  7.01it/s]\n",
      "Pretraining Epochs:  18%|█▊        | 89/500 [00:16<00:57,  7.14it/s]\n",
      "Pretraining Epochs:  18%|█▊        | 90/500 [00:16<00:55,  7.33it/s]\n",
      "Pretraining Epochs:  18%|█▊        | 91/500 [00:16<00:56,  7.29it/s]\n",
      "Pretraining Epochs:  18%|█▊        | 92/500 [00:16<00:57,  7.06it/s]\n",
      "Pretraining Epochs:  19%|█▊        | 93/500 [00:16<01:00,  6.74it/s]\n",
      "Pretraining Epochs:  19%|█▉        | 94/500 [00:16<00:58,  6.93it/s]\n",
      "Pretraining Epochs:  19%|█▉        | 95/500 [00:16<00:59,  6.84it/s]\n",
      "Pretraining Epochs:  19%|█▉        | 96/500 [00:17<00:56,  7.12it/s]\n",
      "Pretraining Epochs:  19%|█▉        | 97/500 [00:17<01:02,  6.49it/s]\n",
      "Pretraining Epochs:  20%|█▉        | 98/500 [00:17<00:58,  6.82it/s]\n",
      "Pretraining Epochs:  20%|█▉        | 99/500 [00:17<00:58,  6.85it/s]\n",
      "Pretraining Epochs:  20%|██        | 100/500 [00:17<00:57,  6.96it/s]\n",
      "Pretraining Epochs:  20%|██        | 101/500 [00:17<00:55,  7.25it/s]\n",
      "Pretraining Epochs:  20%|██        | 102/500 [00:17<00:54,  7.29it/s]\n",
      "Pretraining Epochs:  21%|██        | 103/500 [00:18<00:59,  6.64it/s]\n",
      "Pretraining Epochs:  21%|██        | 104/500 [00:18<00:57,  6.93it/s]\n",
      "Pretraining Epochs:  21%|██        | 105/500 [00:18<00:55,  7.18it/s]\n",
      "Pretraining Epochs:  21%|██        | 106/500 [00:18<00:54,  7.17it/s]\n",
      "Pretraining Epochs:  21%|██▏       | 107/500 [00:18<00:54,  7.20it/s]\n",
      "Pretraining Epochs:  22%|██▏       | 108/500 [00:18<00:54,  7.15it/s]\n",
      "Pretraining Epochs:  22%|██▏       | 109/500 [00:18<00:54,  7.24it/s]\n",
      "Pretraining Epochs:  22%|██▏       | 110/500 [00:19<00:57,  6.84it/s]\n",
      "Pretraining Epochs:  22%|██▏       | 111/500 [00:19<00:58,  6.64it/s]\n",
      "Pretraining Epochs:  22%|██▏       | 112/500 [00:19<00:58,  6.69it/s]\n",
      "Pretraining Epochs:  23%|██▎       | 113/500 [00:19<00:57,  6.70it/s]\n",
      "Pretraining Epochs:  23%|██▎       | 114/500 [00:19<00:55,  6.97it/s]\n",
      "Pretraining Epochs:  23%|██▎       | 115/500 [00:19<00:55,  6.88it/s]\n",
      "Pretraining Epochs:  23%|██▎       | 116/500 [00:19<00:57,  6.65it/s]\n",
      "Pretraining Epochs:  23%|██▎       | 117/500 [00:20<00:56,  6.72it/s]\n",
      "Pretraining Epochs:  24%|██▎       | 118/500 [00:20<00:54,  6.96it/s]\n",
      "Pretraining Epochs:  24%|██▍       | 119/500 [00:20<00:57,  6.63it/s]\n",
      "Pretraining Epochs:  24%|██▍       | 120/500 [00:20<00:57,  6.57it/s]\n",
      "Pretraining Epochs:  24%|██▍       | 121/500 [00:20<00:56,  6.68it/s]\n",
      "Pretraining Epochs:  24%|██▍       | 122/500 [00:20<00:55,  6.82it/s]\n",
      "Pretraining Epochs:  25%|██▍       | 123/500 [00:21<00:57,  6.56it/s]\n",
      "Pretraining Epochs:  25%|██▍       | 124/500 [00:21<00:55,  6.77it/s]\n",
      "Pretraining Epochs:  25%|██▌       | 125/500 [00:21<00:53,  7.04it/s]\n",
      "Pretraining Epochs:  25%|██▌       | 126/500 [00:21<00:52,  7.17it/s]\n",
      "Pretraining Epochs:  25%|██▌       | 127/500 [00:21<00:52,  7.11it/s]\n",
      "Pretraining Epochs:  26%|██▌       | 128/500 [00:21<00:56,  6.59it/s]\n",
      "Pretraining Epochs:  26%|██▌       | 129/500 [00:21<00:55,  6.65it/s]\n",
      "Pretraining Epochs:  26%|██▌       | 130/500 [00:22<00:53,  6.91it/s]\n",
      "Pretraining Epochs:  26%|██▌       | 131/500 [00:22<00:53,  6.91it/s]\n",
      "Pretraining Epochs:  26%|██▋       | 132/500 [00:22<00:51,  7.12it/s]\n",
      "Pretraining Epochs:  27%|██▋       | 133/500 [00:22<00:57,  6.38it/s]\n",
      "Pretraining Epochs:  27%|██▋       | 134/500 [00:22<00:57,  6.36it/s]\n",
      "Pretraining Epochs:  27%|██▋       | 135/500 [00:22<00:55,  6.56it/s]\n",
      "Pretraining Epochs:  27%|██▋       | 136/500 [00:22<00:57,  6.31it/s]\n",
      "Pretraining Epochs:  27%|██▋       | 137/500 [00:23<00:54,  6.66it/s]\n",
      "Pretraining Epochs:  28%|██▊       | 138/500 [00:23<00:52,  6.88it/s]\n",
      "Pretraining Epochs:  28%|██▊       | 139/500 [00:23<00:51,  7.04it/s]\n",
      "Pretraining Epochs:  28%|██▊       | 140/500 [00:23<00:50,  7.18it/s]\n",
      "Pretraining Epochs:  28%|██▊       | 141/500 [00:23<00:51,  7.02it/s]\n",
      "Pretraining Epochs:  28%|██▊       | 142/500 [00:23<00:51,  6.96it/s]\n",
      "Pretraining Epochs:  29%|██▊       | 143/500 [00:23<00:51,  6.98it/s]\n",
      "Pretraining Epochs:  29%|██▉       | 144/500 [00:24<00:51,  6.92it/s]\n",
      "Pretraining Epochs:  29%|██▉       | 145/500 [00:24<00:53,  6.63it/s]\n",
      "Pretraining Epochs:  29%|██▉       | 146/500 [00:24<00:52,  6.71it/s]\n",
      "Pretraining Epochs:  29%|██▉       | 147/500 [00:24<00:51,  6.89it/s]\n",
      "Pretraining Epochs:  30%|██▉       | 148/500 [00:24<00:51,  6.80it/s]\n",
      "Pretraining Epochs:  30%|██▉       | 149/500 [00:24<00:50,  6.90it/s]\n",
      "Pretraining Epochs:  30%|███       | 150/500 [00:24<00:50,  6.92it/s]\n",
      "Pretraining Epochs:  30%|███       | 151/500 [00:25<00:51,  6.82it/s]\n",
      "Pretraining Epochs:  30%|███       | 152/500 [00:25<00:49,  7.04it/s]\n",
      "Pretraining Epochs:  31%|███       | 153/500 [00:25<00:51,  6.80it/s]\n",
      "Pretraining Epochs:  31%|███       | 154/500 [00:25<00:49,  7.02it/s]\n",
      "Pretraining Epochs:  31%|███       | 155/500 [00:25<00:49,  6.92it/s]\n",
      "Pretraining Epochs:  31%|███       | 156/500 [00:25<00:52,  6.52it/s]\n",
      "Pretraining Epochs:  31%|███▏      | 157/500 [00:25<00:51,  6.65it/s]\n",
      "Pretraining Epochs:  32%|███▏      | 158/500 [00:26<00:49,  6.85it/s]\n",
      "Pretraining Epochs:  32%|███▏      | 159/500 [00:26<00:47,  7.16it/s]\n",
      "Pretraining Epochs:  32%|███▏      | 160/500 [00:26<00:48,  7.08it/s]\n",
      "Pretraining Epochs:  32%|███▏      | 161/500 [00:26<00:46,  7.30it/s]\n",
      "Pretraining Epochs:  32%|███▏      | 162/500 [00:26<00:50,  6.65it/s]\n",
      "Pretraining Epochs:  33%|███▎      | 163/500 [00:26<00:48,  6.95it/s]\n",
      "Pretraining Epochs:  33%|███▎      | 164/500 [00:26<00:50,  6.63it/s]\n",
      "Pretraining Epochs:  33%|███▎      | 165/500 [00:27<00:48,  6.89it/s]\n",
      "Pretraining Epochs:  33%|███▎      | 166/500 [00:27<00:47,  7.07it/s]\n",
      "Pretraining Epochs:  33%|███▎      | 167/500 [00:27<00:47,  7.05it/s]\n",
      "Pretraining Epochs:  34%|███▎      | 168/500 [00:27<00:47,  7.01it/s]\n",
      "Pretraining Epochs:  34%|███▍      | 169/500 [00:27<00:46,  7.15it/s]\n",
      "Pretraining Epochs:  34%|███▍      | 170/500 [00:27<00:47,  7.01it/s]\n",
      "Pretraining Epochs:  34%|███▍      | 171/500 [00:27<00:49,  6.71it/s]\n",
      "Pretraining Epochs:  34%|███▍      | 172/500 [00:28<00:46,  7.03it/s]\n",
      "Pretraining Epochs:  35%|███▍      | 173/500 [00:28<00:45,  7.22it/s]\n",
      "Pretraining Epochs:  35%|███▍      | 174/500 [00:28<00:44,  7.26it/s]\n",
      "Pretraining Epochs:  35%|███▌      | 175/500 [00:28<00:47,  6.84it/s]\n",
      "Pretraining Epochs:  35%|███▌      | 176/500 [00:28<00:45,  7.09it/s]\n",
      "Pretraining Epochs:  35%|███▌      | 177/500 [00:28<00:47,  6.84it/s]\n",
      "Pretraining Epochs:  36%|███▌      | 178/500 [00:29<00:52,  6.15it/s]\n",
      "Pretraining Epochs:  36%|███▌      | 179/500 [00:29<00:51,  6.28it/s]\n",
      "Pretraining Epochs:  36%|███▌      | 180/500 [00:29<00:47,  6.68it/s]\n",
      "Pretraining Epochs:  36%|███▌      | 181/500 [00:29<00:45,  6.94it/s]\n",
      "Pretraining Epochs:  36%|███▋      | 182/500 [00:29<00:44,  7.14it/s]\n",
      "Pretraining Epochs:  37%|███▋      | 183/500 [00:29<00:44,  7.15it/s]\n",
      "Pretraining Epochs:  37%|███▋      | 184/500 [00:29<00:43,  7.27it/s]\n",
      "Pretraining Epochs:  37%|███▋      | 185/500 [00:29<00:43,  7.30it/s]\n",
      "Pretraining Epochs:  37%|███▋      | 186/500 [00:30<00:44,  7.09it/s]\n",
      "Pretraining Epochs:  37%|███▋      | 187/500 [00:30<00:44,  7.10it/s]\n",
      "Pretraining Epochs:  38%|███▊      | 188/500 [00:30<00:43,  7.15it/s]\n",
      "Pretraining Epochs:  38%|███▊      | 189/500 [00:30<00:43,  7.12it/s]\n",
      "Pretraining Epochs:  38%|███▊      | 190/500 [00:30<00:42,  7.31it/s]\n",
      "Pretraining Epochs:  38%|███▊      | 191/500 [00:30<00:41,  7.44it/s]\n",
      "Pretraining Epochs:  38%|███▊      | 192/500 [00:30<00:41,  7.33it/s]\n",
      "Pretraining Epochs:  39%|███▊      | 193/500 [00:31<00:43,  7.00it/s]\n",
      "Pretraining Epochs:  39%|███▉      | 194/500 [00:31<00:42,  7.28it/s]\n",
      "Pretraining Epochs:  39%|███▉      | 195/500 [00:31<00:43,  7.08it/s]\n",
      "Pretraining Epochs:  39%|███▉      | 196/500 [00:31<00:42,  7.15it/s]\n",
      "Pretraining Epochs:  39%|███▉      | 197/500 [00:31<00:43,  6.93it/s]\n",
      "Pretraining Epochs:  40%|███▉      | 198/500 [00:31<00:46,  6.43it/s]\n",
      "Pretraining Epochs:  40%|███▉      | 199/500 [00:32<00:47,  6.29it/s]\n",
      "Pretraining Epochs:  40%|████      | 200/500 [00:32<00:45,  6.57it/s]\n",
      "Pretraining Epochs:  40%|████      | 201/500 [00:32<00:48,  6.21it/s]\n",
      "Pretraining Epochs:  40%|████      | 202/500 [00:32<00:45,  6.56it/s]\n",
      "Pretraining Epochs:  41%|████      | 203/500 [00:32<00:43,  6.83it/s]\n",
      "Pretraining Epochs:  41%|████      | 204/500 [00:32<00:42,  6.93it/s]\n",
      "Pretraining Epochs:  41%|████      | 205/500 [00:32<00:42,  6.95it/s]\n",
      "Pretraining Epochs:  41%|████      | 206/500 [00:33<00:42,  6.96it/s]\n",
      "Pretraining Epochs:  41%|████▏     | 207/500 [00:33<00:42,  6.90it/s]\n",
      "Pretraining Epochs:  42%|████▏     | 208/500 [00:33<00:42,  6.89it/s]\n",
      "Pretraining Epochs:  42%|████▏     | 209/500 [00:33<00:43,  6.64it/s]\n",
      "Pretraining Epochs:  42%|████▏     | 210/500 [00:33<00:42,  6.81it/s]\n",
      "Pretraining Epochs:  42%|████▏     | 211/500 [00:33<00:42,  6.85it/s]\n",
      "Pretraining Epochs:  42%|████▏     | 212/500 [00:33<00:44,  6.46it/s]\n",
      "Pretraining Epochs:  43%|████▎     | 213/500 [00:34<00:42,  6.78it/s]\n",
      "Pretraining Epochs:  43%|████▎     | 214/500 [00:34<00:43,  6.54it/s]\n",
      "Pretraining Epochs:  43%|████▎     | 215/500 [00:34<00:42,  6.71it/s]\n",
      "Pretraining Epochs:  43%|████▎     | 216/500 [00:34<00:40,  6.98it/s]\n",
      "Pretraining Epochs:  43%|████▎     | 217/500 [00:34<00:40,  6.99it/s]\n",
      "Pretraining Epochs:  44%|████▎     | 218/500 [00:34<00:39,  7.09it/s]\n",
      "Pretraining Epochs:  44%|████▍     | 219/500 [00:34<00:39,  7.17it/s]\n",
      "Pretraining Epochs:  44%|████▍     | 220/500 [00:35<00:40,  6.98it/s]\n",
      "Pretraining Epochs:  44%|████▍     | 221/500 [00:35<00:40,  6.95it/s]\n",
      "Pretraining Epochs:  44%|████▍     | 222/500 [00:35<00:40,  6.87it/s]\n",
      "Pretraining Epochs:  45%|████▍     | 223/500 [00:35<00:39,  7.10it/s]\n",
      "Pretraining Epochs:  45%|████▍     | 224/500 [00:35<00:41,  6.68it/s]\n",
      "Pretraining Epochs:  45%|████▌     | 225/500 [00:35<00:39,  6.89it/s]\n",
      "Pretraining Epochs:  45%|████▌     | 226/500 [00:35<00:40,  6.71it/s]\n",
      "Pretraining Epochs:  45%|████▌     | 227/500 [00:36<00:41,  6.62it/s]\n",
      "Pretraining Epochs:  46%|████▌     | 228/500 [00:36<00:39,  6.94it/s]\n",
      "Pretraining Epochs:  46%|████▌     | 229/500 [00:36<00:40,  6.73it/s]\n",
      "Pretraining Epochs:  46%|████▌     | 230/500 [00:36<00:41,  6.51it/s]\n",
      "Pretraining Epochs:  46%|████▌     | 231/500 [00:36<00:40,  6.72it/s]\n",
      "Pretraining Epochs:  46%|████▋     | 232/500 [00:36<00:41,  6.39it/s]\n",
      "Pretraining Epochs:  47%|████▋     | 233/500 [00:37<00:43,  6.10it/s]\n",
      "Pretraining Epochs:  47%|████▋     | 234/500 [00:37<00:43,  6.08it/s]\n",
      "Pretraining Epochs:  47%|████▋     | 235/500 [00:37<00:43,  6.15it/s]\n",
      "Pretraining Epochs:  47%|████▋     | 236/500 [00:37<00:41,  6.38it/s]\n",
      "Pretraining Epochs:  47%|████▋     | 237/500 [00:37<00:39,  6.67it/s]\n",
      "Pretraining Epochs:  48%|████▊     | 238/500 [00:37<00:39,  6.58it/s]\n",
      "Pretraining Epochs:  48%|████▊     | 239/500 [00:37<00:40,  6.52it/s]\n",
      "Pretraining Epochs:  48%|████▊     | 240/500 [00:38<00:41,  6.30it/s]\n",
      "Pretraining Epochs:  48%|████▊     | 241/500 [00:38<00:41,  6.28it/s]\n",
      "Pretraining Epochs:  48%|████▊     | 242/500 [00:38<00:39,  6.48it/s]\n",
      "Pretraining Epochs:  49%|████▊     | 243/500 [00:38<00:38,  6.62it/s]\n",
      "Pretraining Epochs:  49%|████▉     | 244/500 [00:38<00:37,  6.76it/s]\n",
      "Pretraining Epochs:  49%|████▉     | 245/500 [00:38<00:37,  6.71it/s]\n",
      "Pretraining Epochs:  49%|████▉     | 246/500 [00:39<00:36,  6.97it/s]\n",
      "Pretraining Epochs:  49%|████▉     | 247/500 [00:39<00:35,  7.12it/s]\n",
      "Pretraining Epochs:  50%|████▉     | 248/500 [00:39<00:37,  6.78it/s]\n",
      "Pretraining Epochs:  50%|████▉     | 249/500 [00:39<00:38,  6.52it/s]\n",
      "Pretraining Epochs:  50%|█████     | 250/500 [00:39<00:37,  6.74it/s]\n",
      "Pretraining Epochs:  50%|█████     | 251/500 [00:39<00:35,  6.92it/s]\n",
      "Pretraining Epochs:  50%|█████     | 252/500 [00:39<00:35,  7.00it/s]\n",
      "Pretraining Epochs:  51%|█████     | 253/500 [00:40<00:34,  7.11it/s]\n",
      "Pretraining Epochs:  51%|█████     | 254/500 [00:40<00:35,  7.02it/s]\n",
      "Pretraining Epochs:  51%|█████     | 255/500 [00:40<00:36,  6.68it/s]\n",
      "Pretraining Epochs:  51%|█████     | 256/500 [00:40<00:34,  7.01it/s]\n",
      "Pretraining Epochs:  51%|█████▏    | 257/500 [00:40<00:35,  6.83it/s]\n",
      "Pretraining Epochs:  52%|█████▏    | 258/500 [00:40<00:33,  7.12it/s]\n",
      "Pretraining Epochs:  52%|█████▏    | 259/500 [00:40<00:33,  7.27it/s]\n",
      "Pretraining Epochs:  52%|█████▏    | 260/500 [00:41<00:35,  6.83it/s]\n",
      "Pretraining Epochs:  52%|█████▏    | 261/500 [00:41<00:34,  6.86it/s]\n",
      "Pretraining Epochs:  52%|█████▏    | 262/500 [00:41<00:34,  6.84it/s]\n",
      "Pretraining Epochs:  53%|█████▎    | 263/500 [00:41<00:33,  7.09it/s]\n",
      "Pretraining Epochs:  53%|█████▎    | 264/500 [00:41<00:34,  6.78it/s]\n",
      "Pretraining Epochs:  53%|█████▎    | 265/500 [00:41<00:33,  7.05it/s]\n",
      "Pretraining Epochs:  53%|█████▎    | 266/500 [00:41<00:32,  7.10it/s]\n",
      "Pretraining Epochs:  53%|█████▎    | 267/500 [00:42<00:31,  7.32it/s]\n",
      "Pretraining Epochs:  54%|█████▎    | 268/500 [00:42<00:31,  7.33it/s]\n",
      "Pretraining Epochs:  54%|█████▍    | 269/500 [00:42<00:31,  7.31it/s]\n",
      "Pretraining Epochs:  54%|█████▍    | 270/500 [00:42<00:30,  7.45it/s]\n",
      "Pretraining Epochs:  54%|█████▍    | 271/500 [00:42<00:29,  7.66it/s]\n",
      "Pretraining Epochs:  54%|█████▍    | 272/500 [00:42<00:30,  7.46it/s]\n",
      "Pretraining Epochs:  55%|█████▍    | 273/500 [00:42<00:30,  7.44it/s]\n",
      "Pretraining Epochs:  55%|█████▍    | 274/500 [00:42<00:30,  7.46it/s]\n",
      "Pretraining Epochs:  55%|█████▌    | 275/500 [00:43<00:33,  6.62it/s]\n",
      "Pretraining Epochs:  55%|█████▌    | 276/500 [00:43<00:34,  6.42it/s]\n",
      "Pretraining Epochs:  55%|█████▌    | 277/500 [00:43<00:36,  6.09it/s]\n",
      "Pretraining Epochs:  56%|█████▌    | 278/500 [00:43<00:35,  6.18it/s]\n",
      "Pretraining Epochs:  56%|█████▌    | 279/500 [00:43<00:35,  6.18it/s]\n",
      "Pretraining Epochs:  56%|█████▌    | 280/500 [00:43<00:34,  6.43it/s]\n",
      "Pretraining Epochs:  56%|█████▌    | 281/500 [00:44<00:33,  6.51it/s]\n",
      "Pretraining Epochs:  56%|█████▋    | 282/500 [00:44<00:33,  6.54it/s]\n",
      "Pretraining Epochs:  57%|█████▋    | 283/500 [00:44<00:32,  6.76it/s]\n",
      "Pretraining Epochs:  57%|█████▋    | 284/500 [00:44<00:30,  6.97it/s]\n",
      "Pretraining Epochs:  57%|█████▋    | 285/500 [00:44<00:32,  6.65it/s]\n",
      "Pretraining Epochs:  57%|█████▋    | 286/500 [00:44<00:33,  6.45it/s]\n",
      "Pretraining Epochs:  57%|█████▋    | 287/500 [00:45<00:32,  6.46it/s]\n",
      "Pretraining Epochs:  58%|█████▊    | 288/500 [00:45<00:32,  6.60it/s]\n",
      "Pretraining Epochs:  58%|█████▊    | 289/500 [00:45<00:31,  6.72it/s]\n",
      "Pretraining Epochs:  58%|█████▊    | 290/500 [00:45<00:33,  6.19it/s]\n",
      "Pretraining Epochs:  58%|█████▊    | 291/500 [00:45<00:32,  6.49it/s]\n",
      "Pretraining Epochs:  58%|█████▊    | 292/500 [00:45<00:30,  6.83it/s]\n",
      "Pretraining Epochs:  59%|█████▊    | 293/500 [00:45<00:29,  7.00it/s]\n",
      "Pretraining Epochs:  59%|█████▉    | 294/500 [00:46<00:29,  7.06it/s]\n",
      "Pretraining Epochs:  59%|█████▉    | 295/500 [00:46<00:28,  7.15it/s]\n",
      "Pretraining Epochs:  59%|█████▉    | 296/500 [00:46<00:29,  6.84it/s]\n",
      "Pretraining Epochs:  59%|█████▉    | 297/500 [00:46<00:28,  7.09it/s]\n",
      "Pretraining Epochs:  60%|█████▉    | 298/500 [00:46<00:29,  6.84it/s]\n",
      "Pretraining Epochs:  60%|█████▉    | 299/500 [00:46<00:28,  7.01it/s]\n",
      "Pretraining Epochs:  60%|██████    | 300/500 [00:46<00:28,  6.94it/s]\n",
      "Pretraining Epochs:  60%|██████    | 301/500 [00:47<00:29,  6.75it/s]\n",
      "Pretraining Epochs:  60%|██████    | 302/500 [00:47<00:28,  6.87it/s]\n",
      "Pretraining Epochs:  61%|██████    | 303/500 [00:47<00:28,  6.99it/s]\n",
      "Pretraining Epochs:  61%|██████    | 304/500 [00:47<00:28,  6.99it/s]\n",
      "Pretraining Epochs:  61%|██████    | 305/500 [00:47<00:27,  7.00it/s]\n",
      "Pretraining Epochs:  61%|██████    | 306/500 [00:47<00:27,  7.09it/s]\n",
      "Pretraining Epochs:  61%|██████▏   | 307/500 [00:47<00:29,  6.62it/s]\n",
      "Pretraining Epochs:  62%|██████▏   | 308/500 [00:48<00:30,  6.24it/s]\n",
      "Pretraining Epochs:  62%|██████▏   | 309/500 [00:48<00:30,  6.37it/s]\n",
      "Pretraining Epochs:  62%|██████▏   | 310/500 [00:48<00:28,  6.72it/s]\n",
      "Pretraining Epochs:  62%|██████▏   | 311/500 [00:48<00:27,  7.00it/s]\n",
      "Pretraining Epochs:  62%|██████▏   | 312/500 [00:48<00:27,  6.91it/s]\n",
      "Pretraining Epochs:  63%|██████▎   | 313/500 [00:48<00:26,  7.12it/s]\n",
      "Pretraining Epochs:  63%|██████▎   | 314/500 [00:48<00:27,  6.69it/s]\n",
      "Pretraining Epochs:  63%|██████▎   | 315/500 [00:49<00:26,  6.99it/s]\n",
      "Pretraining Epochs:  63%|██████▎   | 316/500 [00:49<00:25,  7.08it/s]\n",
      "Pretraining Epochs:  63%|██████▎   | 317/500 [00:49<00:25,  7.09it/s]\n",
      "Pretraining Epochs:  64%|██████▎   | 318/500 [00:49<00:24,  7.33it/s]\n",
      "Pretraining Epochs:  64%|██████▍   | 319/500 [00:49<00:24,  7.24it/s]\n",
      "Pretraining Epochs:  64%|██████▍   | 320/500 [00:49<00:25,  7.08it/s]\n",
      "Pretraining Epochs:  64%|██████▍   | 321/500 [00:49<00:25,  6.90it/s]\n",
      "Pretraining Epochs:  64%|██████▍   | 322/500 [00:50<00:24,  7.22it/s]\n",
      "Pretraining Epochs:  65%|██████▍   | 323/500 [00:50<00:24,  7.31it/s]\n",
      "Pretraining Epochs:  65%|██████▍   | 324/500 [00:50<00:24,  7.21it/s]\n",
      "Pretraining Epochs:  65%|██████▌   | 325/500 [00:50<00:23,  7.31it/s]\n",
      "Pretraining Epochs:  65%|██████▌   | 326/500 [00:50<00:24,  7.21it/s]\n",
      "Pretraining Epochs:  65%|██████▌   | 327/500 [00:50<00:23,  7.35it/s]\n",
      "Pretraining Epochs:  66%|██████▌   | 328/500 [00:50<00:24,  7.13it/s]\n",
      "Pretraining Epochs:  66%|██████▌   | 329/500 [00:51<00:26,  6.50it/s]\n",
      "Pretraining Epochs:  66%|██████▌   | 330/500 [00:51<00:26,  6.51it/s]\n",
      "Pretraining Epochs:  66%|██████▌   | 331/500 [00:51<00:25,  6.60it/s]\n",
      "Pretraining Epochs:  66%|██████▋   | 332/500 [00:51<00:24,  6.85it/s]\n",
      "Pretraining Epochs:  67%|██████▋   | 333/500 [00:51<00:23,  7.05it/s]\n",
      "Pretraining Epochs:  67%|██████▋   | 334/500 [00:51<00:23,  7.17it/s]\n",
      "Pretraining Epochs:  67%|██████▋   | 335/500 [00:51<00:23,  7.14it/s]\n",
      "Pretraining Epochs:  67%|██████▋   | 336/500 [00:52<00:22,  7.26it/s]\n",
      "Pretraining Epochs:  67%|██████▋   | 337/500 [00:52<00:22,  7.21it/s]\n",
      "Pretraining Epochs:  68%|██████▊   | 338/500 [00:52<00:23,  6.95it/s]\n",
      "Pretraining Epochs:  68%|██████▊   | 339/500 [00:52<00:23,  6.96it/s]\n",
      "Pretraining Epochs:  68%|██████▊   | 340/500 [00:52<00:23,  6.93it/s]\n",
      "Pretraining Epochs:  68%|██████▊   | 341/500 [00:52<00:22,  7.11it/s]\n",
      "Pretraining Epochs:  68%|██████▊   | 342/500 [00:52<00:23,  6.78it/s]\n",
      "Pretraining Epochs:  69%|██████▊   | 343/500 [00:53<00:22,  7.04it/s]\n",
      "Pretraining Epochs:  69%|██████▉   | 344/500 [00:53<00:22,  7.08it/s]\n",
      "Pretraining Epochs:  69%|██████▉   | 345/500 [00:53<00:23,  6.54it/s]\n",
      "Pretraining Epochs:  69%|██████▉   | 346/500 [00:53<00:22,  6.90it/s]\n",
      "Pretraining Epochs:  69%|██████▉   | 347/500 [00:53<00:22,  6.88it/s]\n",
      "Pretraining Epochs:  70%|██████▉   | 348/500 [00:53<00:22,  6.74it/s]\n",
      "Pretraining Epochs:  70%|██████▉   | 349/500 [00:53<00:21,  6.94it/s]\n",
      "Pretraining Epochs:  70%|███████   | 350/500 [00:54<00:21,  6.84it/s]\n",
      "Pretraining Epochs:  70%|███████   | 351/500 [00:54<00:21,  6.94it/s]\n",
      "Pretraining Epochs:  70%|███████   | 352/500 [00:54<00:21,  7.01it/s]\n",
      "Pretraining Epochs:  71%|███████   | 353/500 [00:54<00:21,  6.85it/s]\n",
      "Pretraining Epochs:  71%|███████   | 354/500 [00:54<00:21,  6.85it/s]\n",
      "Pretraining Epochs:  71%|███████   | 355/500 [00:54<00:21,  6.90it/s]\n",
      "Pretraining Epochs:  71%|███████   | 356/500 [00:54<00:20,  7.11it/s]\n",
      "Pretraining Epochs:  71%|███████▏  | 357/500 [00:55<00:21,  6.79it/s]\n",
      "Pretraining Epochs:  72%|███████▏  | 358/500 [00:55<00:20,  6.78it/s]\n",
      "Pretraining Epochs:  72%|███████▏  | 359/500 [00:55<00:20,  7.01it/s]\n",
      "Pretraining Epochs:  72%|███████▏  | 360/500 [00:55<00:20,  6.93it/s]\n",
      "Pretraining Epochs:  72%|███████▏  | 361/500 [00:55<00:19,  7.08it/s]\n",
      "Pretraining Epochs:  72%|███████▏  | 362/500 [00:55<00:20,  6.85it/s]\n",
      "Pretraining Epochs:  73%|███████▎  | 363/500 [00:55<00:19,  6.99it/s]\n",
      "Pretraining Epochs:  73%|███████▎  | 364/500 [00:56<00:19,  7.04it/s]\n",
      "Pretraining Epochs:  73%|███████▎  | 365/500 [00:56<00:19,  6.83it/s]\n",
      "Pretraining Epochs:  73%|███████▎  | 366/500 [00:56<00:19,  6.79it/s]\n",
      "Pretraining Epochs:  73%|███████▎  | 367/500 [00:56<00:19,  6.91it/s]\n",
      "Pretraining Epochs:  74%|███████▎  | 368/500 [00:56<00:18,  7.02it/s]\n",
      "Pretraining Epochs:  74%|███████▍  | 369/500 [00:56<00:18,  7.17it/s]\n",
      "Pretraining Epochs:  74%|███████▍  | 370/500 [00:57<00:19,  6.72it/s]\n",
      "Pretraining Epochs:  74%|███████▍  | 371/500 [00:57<00:18,  6.87it/s]\n",
      "Pretraining Epochs:  74%|███████▍  | 372/500 [00:57<00:18,  6.98it/s]\n",
      "Pretraining Epochs:  75%|███████▍  | 373/500 [00:57<00:18,  7.01it/s]\n",
      "Pretraining Epochs:  75%|███████▍  | 374/500 [00:57<00:18,  6.96it/s]\n",
      "Pretraining Epochs:  75%|███████▌  | 375/500 [00:57<00:17,  7.06it/s]\n",
      "Pretraining Epochs:  75%|███████▌  | 376/500 [00:57<00:17,  7.24it/s]\n",
      "Pretraining Epochs:  75%|███████▌  | 377/500 [00:57<00:17,  7.22it/s]\n",
      "Pretraining Epochs:  76%|███████▌  | 378/500 [00:58<00:17,  7.12it/s]\n",
      "Pretraining Epochs:  76%|███████▌  | 379/500 [00:58<00:17,  7.10it/s]\n",
      "Pretraining Epochs:  76%|███████▌  | 380/500 [00:58<00:17,  6.79it/s]\n",
      "Pretraining Epochs:  76%|███████▌  | 381/500 [00:58<00:16,  7.06it/s]\n",
      "Pretraining Epochs:  76%|███████▋  | 382/500 [00:58<00:16,  7.16it/s]\n",
      "Pretraining Epochs:  77%|███████▋  | 383/500 [00:58<00:16,  7.01it/s]\n",
      "Pretraining Epochs:  77%|███████▋  | 384/500 [00:58<00:16,  7.00it/s]\n",
      "Pretraining Epochs:  77%|███████▋  | 385/500 [00:59<00:17,  6.51it/s]\n",
      "Pretraining Epochs:  77%|███████▋  | 386/500 [00:59<00:16,  6.74it/s]\n",
      "Pretraining Epochs:  77%|███████▋  | 387/500 [00:59<00:17,  6.51it/s]\n",
      "Pretraining Epochs:  78%|███████▊  | 388/500 [00:59<00:17,  6.35it/s]\n",
      "Pretraining Epochs:  78%|███████▊  | 389/500 [00:59<00:16,  6.71it/s]\n",
      "Pretraining Epochs:  78%|███████▊  | 390/500 [00:59<00:16,  6.73it/s]\n",
      "Pretraining Epochs:  78%|███████▊  | 391/500 [01:00<00:15,  7.01it/s]\n",
      "Pretraining Epochs:  78%|███████▊  | 392/500 [01:00<00:15,  6.79it/s]\n",
      "Pretraining Epochs:  79%|███████▊  | 393/500 [01:00<00:15,  6.76it/s]\n",
      "Pretraining Epochs:  79%|███████▉  | 394/500 [01:00<00:15,  6.92it/s]\n",
      "Pretraining Epochs:  79%|███████▉  | 395/500 [01:00<00:15,  6.66it/s]\n",
      "Pretraining Epochs:  79%|███████▉  | 396/500 [01:00<00:16,  6.21it/s]\n",
      "Pretraining Epochs:  79%|███████▉  | 397/500 [01:00<00:15,  6.57it/s]\n",
      "Pretraining Epochs:  80%|███████▉  | 398/500 [01:01<00:15,  6.59it/s]\n",
      "Pretraining Epochs:  80%|███████▉  | 399/500 [01:01<00:14,  6.88it/s]\n",
      "Pretraining Epochs:  80%|████████  | 400/500 [01:01<00:14,  6.94it/s]\n",
      "Pretraining Epochs:  80%|████████  | 401/500 [01:01<00:13,  7.24it/s]\n",
      "Pretraining Epochs:  80%|████████  | 402/500 [01:01<00:14,  6.94it/s]\n",
      "Pretraining Epochs:  81%|████████  | 403/500 [01:01<00:14,  6.74it/s]\n",
      "Pretraining Epochs:  81%|████████  | 404/500 [01:01<00:14,  6.76it/s]\n",
      "Pretraining Epochs:  81%|████████  | 405/500 [01:02<00:14,  6.45it/s]\n",
      "Pretraining Epochs:  81%|████████  | 406/500 [01:02<00:13,  6.81it/s]\n",
      "Pretraining Epochs:  81%|████████▏ | 407/500 [01:02<00:13,  7.02it/s]\n",
      "Pretraining Epochs:  82%|████████▏ | 408/500 [01:02<00:12,  7.25it/s]\n",
      "Pretraining Epochs:  82%|████████▏ | 409/500 [01:02<00:13,  6.93it/s]\n",
      "Pretraining Epochs:  82%|████████▏ | 410/500 [01:02<00:12,  6.95it/s]\n",
      "Pretraining Epochs:  82%|████████▏ | 411/500 [01:02<00:12,  6.91it/s]\n",
      "Pretraining Epochs:  82%|████████▏ | 412/500 [01:03<00:12,  7.09it/s]\n",
      "Pretraining Epochs:  83%|████████▎ | 413/500 [01:03<00:11,  7.29it/s]\n",
      "Pretraining Epochs:  83%|████████▎ | 414/500 [01:03<00:12,  6.88it/s]\n",
      "Pretraining Epochs:  83%|████████▎ | 415/500 [01:03<00:12,  6.79it/s]\n",
      "Pretraining Epochs:  83%|████████▎ | 416/500 [01:03<00:12,  6.83it/s]\n",
      "Pretraining Epochs:  83%|████████▎ | 417/500 [01:03<00:12,  6.57it/s]\n",
      "Pretraining Epochs:  84%|████████▎ | 418/500 [01:03<00:12,  6.81it/s]\n",
      "Pretraining Epochs:  84%|████████▍ | 419/500 [01:04<00:11,  6.78it/s]\n",
      "Pretraining Epochs:  84%|████████▍ | 420/500 [01:04<00:11,  6.90it/s]\n",
      "Pretraining Epochs:  84%|████████▍ | 421/500 [01:04<00:11,  7.03it/s]\n",
      "Pretraining Epochs:  84%|████████▍ | 422/500 [01:04<00:10,  7.22it/s]\n",
      "Pretraining Epochs:  85%|████████▍ | 423/500 [01:04<00:10,  7.26it/s]\n",
      "Pretraining Epochs:  85%|████████▍ | 424/500 [01:04<00:10,  7.36it/s]\n",
      "Pretraining Epochs:  85%|████████▌ | 425/500 [01:04<00:10,  7.35it/s]\n",
      "Pretraining Epochs:  85%|████████▌ | 426/500 [01:05<00:10,  7.23it/s]\n",
      "Pretraining Epochs:  85%|████████▌ | 427/500 [01:05<00:09,  7.48it/s]\n",
      "Pretraining Epochs:  86%|████████▌ | 428/500 [01:05<00:09,  7.36it/s]\n",
      "Pretraining Epochs:  86%|████████▌ | 429/500 [01:05<00:09,  7.39it/s]\n",
      "Pretraining Epochs:  86%|████████▌ | 430/500 [01:05<00:09,  7.33it/s]\n",
      "Pretraining Epochs:  86%|████████▌ | 431/500 [01:05<00:09,  7.29it/s]\n",
      "Pretraining Epochs:  86%|████████▋ | 432/500 [01:05<00:09,  7.26it/s]\n",
      "Pretraining Epochs:  87%|████████▋ | 433/500 [01:06<00:10,  6.54it/s]\n",
      "Pretraining Epochs:  87%|████████▋ | 434/500 [01:06<00:10,  6.54it/s]\n",
      "Pretraining Epochs:  87%|████████▋ | 435/500 [01:06<00:09,  6.87it/s]\n",
      "Pretraining Epochs:  87%|████████▋ | 436/500 [01:06<00:09,  7.08it/s]\n",
      "Pretraining Epochs:  87%|████████▋ | 437/500 [01:06<00:08,  7.10it/s]\n",
      "Pretraining Epochs:  88%|████████▊ | 438/500 [01:06<00:08,  6.92it/s]\n",
      "Pretraining Epochs:  88%|████████▊ | 439/500 [01:06<00:09,  6.47it/s]\n",
      "Pretraining Epochs:  88%|████████▊ | 440/500 [01:07<00:08,  6.81it/s]\n",
      "Pretraining Epochs:  88%|████████▊ | 441/500 [01:07<00:08,  6.84it/s]\n",
      "Pretraining Epochs:  88%|████████▊ | 442/500 [01:07<00:08,  6.89it/s]\n",
      "Pretraining Epochs:  89%|████████▊ | 443/500 [01:07<00:08,  6.90it/s]\n",
      "Pretraining Epochs:  89%|████████▉ | 444/500 [01:07<00:07,  7.12it/s]\n",
      "Pretraining Epochs:  89%|████████▉ | 445/500 [01:07<00:07,  7.08it/s]\n",
      "Pretraining Epochs:  89%|████████▉ | 446/500 [01:07<00:07,  7.25it/s]\n",
      "Pretraining Epochs:  89%|████████▉ | 447/500 [01:08<00:07,  7.26it/s]\n",
      "Pretraining Epochs:  90%|████████▉ | 448/500 [01:08<00:07,  7.37it/s]\n",
      "Pretraining Epochs:  90%|████████▉ | 449/500 [01:08<00:06,  7.31it/s]\n",
      "Pretraining Epochs:  90%|█████████ | 450/500 [01:08<00:06,  7.35it/s]\n",
      "Pretraining Epochs:  90%|█████████ | 451/500 [01:08<00:07,  6.91it/s]\n",
      "Pretraining Epochs:  90%|█████████ | 452/500 [01:08<00:06,  6.96it/s]\n",
      "Pretraining Epochs:  91%|█████████ | 453/500 [01:08<00:06,  7.05it/s]\n",
      "Pretraining Epochs:  91%|█████████ | 454/500 [01:09<00:06,  6.78it/s]\n",
      "Pretraining Epochs:  91%|█████████ | 455/500 [01:09<00:06,  6.49it/s]\n",
      "Pretraining Epochs:  91%|█████████ | 456/500 [01:09<00:06,  6.85it/s]\n",
      "Pretraining Epochs:  91%|█████████▏| 457/500 [01:09<00:06,  7.09it/s]\n",
      "Pretraining Epochs:  92%|█████████▏| 458/500 [01:09<00:05,  7.21it/s]\n",
      "Pretraining Epochs:  92%|█████████▏| 459/500 [01:09<00:05,  7.33it/s]\n",
      "Pretraining Epochs:  92%|█████████▏| 460/500 [01:09<00:05,  7.29it/s]\n",
      "Pretraining Epochs:  92%|█████████▏| 461/500 [01:10<00:05,  7.11it/s]\n",
      "Pretraining Epochs:  92%|█████████▏| 462/500 [01:10<00:05,  7.12it/s]\n",
      "Pretraining Epochs:  93%|█████████▎| 463/500 [01:10<00:05,  7.30it/s]\n",
      "Pretraining Epochs:  93%|█████████▎| 464/500 [01:10<00:05,  6.44it/s]\n",
      "Pretraining Epochs:  93%|█████████▎| 465/500 [01:10<00:05,  6.71it/s]\n",
      "Pretraining Epochs:  93%|█████████▎| 466/500 [01:10<00:05,  6.74it/s]\n",
      "Pretraining Epochs:  93%|█████████▎| 467/500 [01:10<00:04,  6.73it/s]\n",
      "Pretraining Epochs:  94%|█████████▎| 468/500 [01:11<00:04,  6.59it/s]\n",
      "Pretraining Epochs:  94%|█████████▍| 469/500 [01:11<00:04,  6.77it/s]\n",
      "Pretraining Epochs:  94%|█████████▍| 470/500 [01:11<00:04,  6.94it/s]\n",
      "Pretraining Epochs:  94%|█████████▍| 471/500 [01:11<00:04,  7.11it/s]\n",
      "Pretraining Epochs:  94%|█████████▍| 472/500 [01:11<00:04,  6.91it/s]\n",
      "Pretraining Epochs:  95%|█████████▍| 473/500 [01:11<00:04,  6.48it/s]\n",
      "Pretraining Epochs:  95%|█████████▍| 474/500 [01:12<00:03,  6.71it/s]\n",
      "Pretraining Epochs:  95%|█████████▌| 475/500 [01:12<00:03,  6.64it/s]\n",
      "Pretraining Epochs:  95%|█████████▌| 476/500 [01:12<00:03,  6.75it/s]\n",
      "Pretraining Epochs:  95%|█████████▌| 477/500 [01:12<00:03,  7.00it/s]\n",
      "Pretraining Epochs:  96%|█████████▌| 478/500 [01:12<00:03,  6.72it/s]\n",
      "Pretraining Epochs:  96%|█████████▌| 479/500 [01:12<00:03,  6.79it/s]\n",
      "Pretraining Epochs:  96%|█████████▌| 480/500 [01:12<00:02,  6.78it/s]\n",
      "Pretraining Epochs:  96%|█████████▌| 481/500 [01:13<00:02,  6.52it/s]\n",
      "Pretraining Epochs:  96%|█████████▋| 482/500 [01:13<00:02,  6.68it/s]\n",
      "Pretraining Epochs:  97%|█████████▋| 483/500 [01:13<00:02,  6.77it/s]\n",
      "Pretraining Epochs:  97%|█████████▋| 484/500 [01:13<00:02,  6.74it/s]\n",
      "Pretraining Epochs:  97%|█████████▋| 485/500 [01:13<00:02,  6.96it/s]\n",
      "Pretraining Epochs:  97%|█████████▋| 486/500 [01:13<00:01,  7.18it/s]\n",
      "Pretraining Epochs:  97%|█████████▋| 487/500 [01:13<00:01,  7.28it/s]\n",
      "Pretraining Epochs:  98%|█████████▊| 488/500 [01:14<00:01,  7.21it/s]\n",
      "Pretraining Epochs:  98%|█████████▊| 489/500 [01:14<00:01,  6.27it/s]\n",
      "Pretraining Epochs:  98%|█████████▊| 490/500 [01:14<00:01,  6.69it/s]\n",
      "Pretraining Epochs:  98%|█████████▊| 491/500 [01:14<00:01,  6.98it/s]\n",
      "Pretraining Epochs:  98%|█████████▊| 492/500 [01:14<00:01,  6.78it/s]\n",
      "Pretraining Epochs:  99%|█████████▊| 493/500 [01:14<00:01,  6.70it/s]\n",
      "Pretraining Epochs:  99%|█████████▉| 494/500 [01:14<00:00,  6.52it/s]\n",
      "Pretraining Epochs:  99%|█████████▉| 495/500 [01:15<00:00,  6.76it/s]\n",
      "Pretraining Epochs:  99%|█████████▉| 496/500 [01:15<00:00,  7.03it/s]\n",
      "Pretraining Epochs:  99%|█████████▉| 497/500 [01:15<00:00,  7.03it/s]\n",
      "Pretraining Epochs: 100%|█████████▉| 498/500 [01:15<00:00,  7.01it/s]\n",
      "Pretraining Epochs: 100%|█████████▉| 499/500 [01:15<00:00,  7.23it/s]\n",
      "Pretraining Epochs: 100%|██████████| 500/500 [01:15<00:00,  7.30it/s]\n",
      "Pretraining Epochs: 100%|██████████| 500/500 [01:15<00:00,  6.60it/s]\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "  --dataset bitcoin_alpha \\\n",
    "  --mode pretrain \\\n",
    "  --train_file experiment-data/bitcoin_alpha-train-1.edgelist \\\n",
    "  --device cuda \\\n",
    "  --epochs 500 \\\n",
    "  --node_feat_dim 16 \\\n",
    "  --embed_dim 16 \\\n",
    "  --num_heads 4 \\\n",
    "  --num_layers 2 \\\n",
    "  --dropout_rate 0.1 \\\n",
    "  --lr 0.001 \\\n",
    "  --weight_decay 0.0001 \\\n",
    "  --pretrain_path embeddings/bitcoin_alpha_ca_sdgnn_pretrained.pth \\\n",
    "  --output_dir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63364,
     "status": "ok",
     "timestamp": 1744884075800,
     "user": {
      "displayName": "Abhin B",
      "userId": "18374123182166733868"
     },
     "user_tz": -330
    },
    "id": "__HFEC2OGaPe",
    "outputId": "8b455c4a-62fd-4787-da68-f20f4383ba4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\abhin\\Comding\\ML\\Capstone\\casdgnn\\data_utils.py:138: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:656.)\n",
      "  signed_adj_matrix = torch.sparse.FloatTensor(indices, values, (num_nodes, num_nodes))\n",
      "\n",
      "Fine-tuning Epochs:   0%|          | 0/50 [00:00<?, ?it/s]\n",
      "Fine-tuning Epochs:   2%|▏         | 1/50 [00:36<29:55, 36.64s/it]\n",
      "Fine-tuning Epochs:   4%|▍         | 2/50 [01:11<28:33, 35.70s/it]\n",
      "Fine-tuning Epochs:   6%|▌         | 3/50 [01:45<27:13, 34.76s/it]\n",
      "Fine-tuning Epochs:   8%|▊         | 4/50 [02:25<28:10, 36.76s/it]\n",
      "Fine-tuning Epochs:  10%|█         | 5/50 [03:01<27:32, 36.73s/it]\n",
      "Fine-tuning Epochs:  12%|█▏        | 6/50 [03:38<26:55, 36.72s/it]\n",
      "Fine-tuning Epochs:  14%|█▍        | 7/50 [04:15<26:19, 36.73s/it]\n",
      "Fine-tuning Epochs:  16%|█▌        | 8/50 [04:46<24:28, 34.96s/it]\n",
      "Fine-tuning Epochs:  18%|█▊        | 9/50 [05:17<23:03, 33.75s/it]\n",
      "Fine-tuning Epochs:  20%|██        | 10/50 [05:50<22:23, 33.58s/it]\n",
      "Fine-tuning Epochs:  22%|██▏       | 11/50 [06:19<20:55, 32.19s/it]\n",
      "Fine-tuning Epochs:  24%|██▍       | 12/50 [06:48<19:45, 31.19s/it]\n",
      "Fine-tuning Epochs:  26%|██▌       | 13/50 [07:24<20:02, 32.50s/it]\n",
      "Fine-tuning Epochs:  28%|██▊       | 14/50 [07:58<19:52, 33.13s/it]\n",
      "Fine-tuning Epochs:  30%|███       | 15/50 [08:38<20:26, 35.06s/it]\n",
      "Fine-tuning Epochs:  32%|███▏      | 16/50 [09:14<20:06, 35.47s/it]\n",
      "Fine-tuning Epochs:  34%|███▍      | 17/50 [09:45<18:45, 34.10s/it]\n",
      "Fine-tuning Epochs:  36%|███▌      | 18/50 [10:17<17:50, 33.47s/it]\n",
      "Fine-tuning Epochs:  38%|███▊      | 19/50 [10:48<16:57, 32.81s/it]\n",
      "Fine-tuning Epochs:  40%|████      | 20/50 [11:24<16:47, 33.57s/it]\n",
      "Fine-tuning Epochs:  42%|████▏     | 21/50 [11:59<16:29, 34.13s/it]\n",
      "Fine-tuning Epochs:  44%|████▍     | 22/50 [12:30<15:30, 33.24s/it]\n",
      "Fine-tuning Epochs:  46%|████▌     | 23/50 [13:01<14:33, 32.36s/it]\n",
      "Fine-tuning Epochs:  48%|████▊     | 24/50 [13:31<13:49, 31.90s/it]\n",
      "Fine-tuning Epochs:  50%|█████     | 25/50 [14:03<13:17, 31.91s/it]\n",
      "Fine-tuning Epochs:  52%|█████▏    | 26/50 [14:38<13:03, 32.63s/it]\n",
      "Fine-tuning Epochs:  54%|█████▍    | 27/50 [15:10<12:24, 32.39s/it]\n",
      "Fine-tuning Epochs:  56%|█████▌    | 28/50 [15:39<11:34, 31.56s/it]\n",
      "Fine-tuning Epochs:  58%|█████▊    | 29/50 [16:09<10:54, 31.14s/it]\n",
      "Fine-tuning Epochs:  60%|██████    | 30/50 [16:40<10:17, 30.90s/it]\n",
      "Fine-tuning Epochs:  62%|██████▏   | 31/50 [17:08<09:30, 30.02s/it]\n",
      "Fine-tuning Epochs:  64%|██████▍   | 32/50 [17:37<08:55, 29.75s/it]\n",
      "Fine-tuning Epochs:  66%|██████▌   | 33/50 [18:05<08:17, 29.25s/it]\n",
      "Fine-tuning Epochs:  68%|██████▊   | 34/50 [18:37<08:04, 30.26s/it]\n",
      "Fine-tuning Epochs:  70%|███████   | 35/50 [19:10<07:46, 31.07s/it]\n",
      "Fine-tuning Epochs:  72%|███████▏  | 36/50 [19:40<07:07, 30.53s/it]\n",
      "Fine-tuning Epochs:  74%|███████▍  | 37/50 [20:12<06:43, 31.07s/it]\n",
      "Fine-tuning Epochs:  76%|███████▌  | 38/50 [20:47<06:25, 32.14s/it]\n",
      "Fine-tuning Epochs:  78%|███████▊  | 39/50 [21:19<05:54, 32.25s/it]\n",
      "Fine-tuning Epochs:  80%|████████  | 40/50 [21:56<05:37, 33.70s/it]\n",
      "Fine-tuning Epochs:  82%|████████▏ | 41/50 [22:34<05:13, 34.80s/it]\n",
      "Fine-tuning Epochs:  84%|████████▍ | 42/50 [23:09<04:39, 34.99s/it]\n",
      "Fine-tuning Epochs:  86%|████████▌ | 43/50 [23:42<04:00, 34.34s/it]\n",
      "Fine-tuning Epochs:  88%|████████▊ | 44/50 [24:15<03:24, 34.07s/it]\n",
      "Fine-tuning Epochs:  90%|█████████ | 45/50 [24:47<02:47, 33.49s/it]\n",
      "Fine-tuning Epochs:  92%|█████████▏| 46/50 [25:19<02:11, 32.97s/it]\n",
      "Fine-tuning Epochs:  94%|█████████▍| 47/50 [25:49<01:36, 32.17s/it]\n",
      "Fine-tuning Epochs:  96%|█████████▌| 48/50 [26:23<01:04, 32.44s/it]\n",
      "Fine-tuning Epochs:  98%|█████████▊| 49/50 [26:55<00:32, 32.43s/it]\n",
      "Fine-tuning Epochs: 100%|██████████| 50/50 [27:27<00:00, 32.24s/it]\n",
      "Fine-tuning Epochs: 100%|██████████| 50/50 [27:27<00:00, 32.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce GTX 1650 with Max-Q Design (torch.cuda.is_available()=True)\n",
      "[LOG] Parsing arguments...\n",
      "[LOG] Setting random seeds...\n",
      "[LOG] Loading training edge list...\n",
      "[LOG] Number of nodes: 3650\n",
      "[LOG] Creating adjacency lists...\n",
      "[LOG] Computing centrality features and node sign influence...\n",
      "[LOG] Creating adjacency matrices...\n",
      "[LOG] Initializing node features...\n",
      "[LOG] Initializing model...\n",
      "[LOG] Setting up optimizer...\n",
      "[LOG] Starting fine-tuning phase...\n",
      "[LOG] Loading pretrained model from embeddings/bitcoin_alpha_ca_sdgnn_pretrained.pth...\n",
      "[LOG] Loaded pretrained model from embeddings/bitcoin_alpha_ca_sdgnn_pretrained.pth\n",
      "[LOG] Preparing edge list and labels for fine-tuning...\n",
      "[LOG] Computing weight_dict for fine-tuning loss...\n",
      "[LOG] Calling finetune()...\n",
      "Epoch [1/50], Fine-tuning Loss: 25340.4004\n",
      "Epoch [2/50], Fine-tuning Loss: 24021.7344\n",
      "Epoch [3/50], Fine-tuning Loss: 22774.5430\n",
      "Epoch [4/50], Fine-tuning Loss: 21233.8887\n",
      "Epoch [5/50], Fine-tuning Loss: 19989.4844\n",
      "Epoch [6/50], Fine-tuning Loss: 19089.4180\n",
      "Epoch [7/50], Fine-tuning Loss: 17815.8809\n",
      "Epoch [8/50], Fine-tuning Loss: 16771.2070\n",
      "Epoch [9/50], Fine-tuning Loss: 16061.3525\n",
      "Epoch [10/50], Fine-tuning Loss: 14749.3945\n",
      "Epoch [11/50], Fine-tuning Loss: 13880.2920\n",
      "Epoch [12/50], Fine-tuning Loss: 13068.4365\n",
      "Epoch [13/50], Fine-tuning Loss: 12415.5713\n",
      "Epoch [14/50], Fine-tuning Loss: 11997.4619\n",
      "Epoch [15/50], Fine-tuning Loss: 11200.6797\n",
      "Epoch [16/50], Fine-tuning Loss: 10507.1152\n",
      "Epoch [17/50], Fine-tuning Loss: 10083.5322\n",
      "Epoch [18/50], Fine-tuning Loss: 9642.3555\n",
      "Epoch [19/50], Fine-tuning Loss: 9373.2715\n",
      "Epoch [20/50], Fine-tuning Loss: 8956.6816\n",
      "Epoch [21/50], Fine-tuning Loss: 8655.8809\n",
      "Epoch [22/50], Fine-tuning Loss: 8499.1045\n",
      "Epoch [23/50], Fine-tuning Loss: 8285.1035\n",
      "Epoch [24/50], Fine-tuning Loss: 8154.0132\n",
      "Epoch [25/50], Fine-tuning Loss: 7911.2500\n",
      "Epoch [26/50], Fine-tuning Loss: 7729.3003\n",
      "Epoch [27/50], Fine-tuning Loss: 7594.5469\n",
      "Epoch [28/50], Fine-tuning Loss: 7361.6519\n",
      "Epoch [29/50], Fine-tuning Loss: 7072.0371\n",
      "Epoch [30/50], Fine-tuning Loss: 7151.3774\n",
      "Epoch [31/50], Fine-tuning Loss: 7033.9155\n",
      "Epoch [32/50], Fine-tuning Loss: 6875.9507\n",
      "Epoch [33/50], Fine-tuning Loss: 6871.9224\n",
      "Epoch [34/50], Fine-tuning Loss: 6827.5947\n",
      "Epoch [35/50], Fine-tuning Loss: 6701.0859\n",
      "Epoch [36/50], Fine-tuning Loss: 6514.1118\n",
      "Epoch [37/50], Fine-tuning Loss: 6279.4468\n",
      "Epoch [38/50], Fine-tuning Loss: 6322.2109\n",
      "Epoch [39/50], Fine-tuning Loss: 6306.4644\n",
      "Epoch [40/50], Fine-tuning Loss: 6219.0220\n",
      "Epoch [41/50], Fine-tuning Loss: 5938.9160\n",
      "Epoch [42/50], Fine-tuning Loss: 5746.6411\n",
      "Epoch [43/50], Fine-tuning Loss: 5870.4233\n",
      "Epoch [44/50], Fine-tuning Loss: 5806.1084\n",
      "Epoch [45/50], Fine-tuning Loss: 5585.1118\n",
      "Epoch [46/50], Fine-tuning Loss: 5654.3691\n",
      "Epoch [47/50], Fine-tuning Loss: 5405.7998\n",
      "Epoch [48/50], Fine-tuning Loss: 5278.6074\n",
      "Epoch [49/50], Fine-tuning Loss: 5175.4116\n",
      "Epoch [50/50], Fine-tuning Loss: 5182.1367\n",
      "Fine-tuned model saved to embeddings/bitcoin_alpha_ca_sdgnn_finetuned.pth\n",
      "[LOG] Fine-tuned model saved to embeddings/bitcoin_alpha_ca_sdgnn_finetuned.pth\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "  --dataset bitcoin_alpha \\\n",
    "  --mode finetune \\\n",
    "  --train_file experiment-data/bitcoin_alpha-train-1.edgelist \\\n",
    "  --device cuda \\\n",
    "  --epochs 50 \\\n",
    "  --node_feat_dim 16 \\\n",
    "  --embed_dim 16 \\\n",
    "  --num_heads 4 \\\n",
    "  --num_layers 2 \\\n",
    "  --dropout_rate 0.1 \\\n",
    "  --lr 0.0005 \\\n",
    "  --weight_decay 0.0001 \\\n",
    "  --pretrain_path embeddings/bitcoin_alpha_ca_sdgnn_pretrained.pth \\\n",
    "  --finetune_path embeddings/bitcoin_alpha_ca_sdgnn_finetuned.pth \\\n",
    "  --output_dir output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Testing\n",
    "\n",
    "After pretraining and fine-tuning, we evaluate the model on the validation and test sets. Adjust the file paths and parameters as needed for your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce GTX 1650 with Max-Q Design (torch.cuda.is_available()=True)\n",
      "[LOG] Parsing arguments...\n",
      "[LOG] Setting random seeds...\n",
      "[LOG] Loading training edge list...\n",
      "[LOG] Number of nodes: 3650\n",
      "[LOG] Creating adjacency lists...\n",
      "[LOG] Computing centrality features and node sign influence...\n",
      "[LOG] Creating adjacency matrices...\n",
      "[LOG] Initializing node features...\n",
      "[LOG] Initializing model...\n",
      "[LOG] Setting up optimizer...\n",
      "[LOG] Starting inference phase...\n",
      "[LOG] Loading fine-tuned model from embeddings/bitcoin_alpha_ca_sdgnn_finetuned.pth...\n",
      "[LOG] Loaded fine-tuned model from embeddings/bitcoin_alpha_ca_sdgnn_finetuned.pth\n",
      "[LOG] Loading and preparing test edge list...\n",
      "[LOG] Running inference...\n",
      "Optimal threshold: 0.10\n",
      "Accuracy: 0.9394\n",
      "Precision: 0.9394\n",
      "Recall: 1.0000\n",
      "Binary F1 Score: 0.9688\n",
      "Micro F1 Score: 0.9394\n",
      "Macro F1 Score: 0.4844\n",
      "AUC: 0.5133\n",
      "Predictions saved to output\\predictions.txt\n",
      "Evaluation Metrics: {'accuracy': 0.9394377842083506, 'precision': 0.9394377842083506, 'recall': 1.0, 'binary_f1': 0.9687733134391986, 'micro_f1': 0.9394377842083506, 'macro_f1': 0.4843866567195993, 'auc': 0.5132790412147017, 'optimal_threshold': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\abhin\\Comding\\ML\\Capstone\\casdgnn\\data_utils.py:138: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:656.)\n",
      "  signed_adj_matrix = torch.sparse.FloatTensor(indices, values, (num_nodes, num_nodes))\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "  --dataset bitcoin_alpha \\\n",
    "  --mode infer \\\n",
    "  --train_file experiment-data/bitcoin_alpha-train-1.edgelist \\\n",
    "  --test_file experiment-data/bitcoin_alpha-test-1.edgelist \\\n",
    "  --device cuda \\\n",
    "  --node_feat_dim 16 \\\n",
    "  --embed_dim 16 \\\n",
    "  --num_heads 4 \\\n",
    "  --num_layers 2 \\\n",
    "  --dropout_rate 0.1 \\\n",
    "  --pretrain_path embeddings/bitcoin_alpha_ca_sdgnn_pretrained.pth \\\n",
    "  --finetune_path embeddings/bitcoin_alpha_ca_sdgnn_finetuned.pth \\\n",
    "  --output_dir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py \\\n",
    "  --dataset bitcoin_alpha \\\n",
    "  --mode test \\\n",
    "  --train_file experiment-data/bitcoin_alpha-train-1.edgelist \\\n",
    "  --test_file experiment-data/bitcoin_alpha-test-1.edgelist \\\n",
    "  --device cuda \\\n",
    "  --node_feat_dim 16 \\\n",
    "  --embed_dim 16 \\\n",
    "  --num_heads 4 \\\n",
    "  --num_layers 2 \\\n",
    "  --dropout_rate 0.1 \\\n",
    "  --pretrain_path embeddings/bitcoin_alpha_ca_sdgnn_pretrained.pth \\\n",
    "  --finetune_path embeddings/bitcoin_alpha_ca_sdgnn_finetuned.pth \\\n",
    "  --output_dir output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Results\n",
    "\n",
    "Check the `output` directory for logs, metrics, and model outputs. You can visualize or further process these results as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running in Google Colab\n",
    "\n",
    "If you are running this notebook in Google Colab, use the following cells to set up your environment and adjust file paths accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set your project root directory (update this path as needed)\n",
    "project_root = '/content/drive/MyDrive/Capstone/casdgnn'\n",
    "os.chdir(project_root)\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Pretraining in Colab\n",
    "!python main.py \\\n",
    "  --dataset bitcoin_alpha \\\n",
    "  --mode pretrain \\\n",
    "  --train_file experiment-data/bitcoin_alpha-train-1.edgelist \\\n",
    "  --device cuda \\\n",
    "  --epochs 500 \\\n",
    "  --node_feat_dim 16 \\\n",
    "  --embed_dim 16 \\\n",
    "  --num_heads 4 \\\n",
    "  --num_layers 2 \\\n",
    "  --dropout_rate 0.1 \\\n",
    "  --lr 0.001 \\\n",
    "  --weight_decay 0.0001 \\\n",
    "  --pretrain_path embeddings/bitcoin_alpha_ca_sdgnn_pretrained.pth \\\n",
    "  --output_dir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Fine-tuning in Colab\n",
    "!python main.py \\\n",
    "  --dataset bitcoin_alpha \\\n",
    "  --mode finetune \\\n",
    "  --train_file experiment-data/bitcoin_alpha-train-1.edgelist \\\n",
    "  --device cuda \\\n",
    "  --epochs 50 \\\n",
    "  --node_feat_dim 16 \\\n",
    "  --embed_dim 16 \\\n",
    "  --num_heads 4 \\\n",
    "  --num_layers 2 \\\n",
    "  --dropout_rate 0.1 \\\n",
    "  --lr 0.0005 \\\n",
    "  --weight_decay 0.0001 \\\n",
    "  --pretrain_path embeddings/bitcoin_alpha_ca_sdgnn_pretrained.pth \\\n",
    "  --finetune_path embeddings/bitcoin_alpha_ca_sdgnn_finetuned.pth \\\n",
    "  --output_dir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Validation in Colab\n",
    "!python main.py \\\n",
    "  --dataset bitcoin_alpha \\\n",
    "  --mode validate \\\n",
    "  --train_file experiment-data/bitcoin_alpha-train-1.edgelist \\\n",
    "  --val_file experiment-data/bitcoin_alpha-test-1.edgelist \\\n",
    "  --device cuda \\\n",
    "  --node_feat_dim 16 \\\n",
    "  --embed_dim 16 \\\n",
    "  --num_heads 4 \\\n",
    "  --num_layers 2 \\\n",
    "  --dropout_rate 0.1 \\\n",
    "  --pretrain_path embeddings/bitcoin_alpha_ca_sdgnn_pretrained.pth \\\n",
    "  --finetune_path embeddings/bitcoin_alpha_ca_sdgnn_finetuned.pth \\\n",
    "  --output_dir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Testing in Colab\n",
    "!python main.py \\\n",
    "  --dataset bitcoin_alpha \\\n",
    "  --mode test \\\n",
    "  --train_file experiment-data/bitcoin_alpha-train-1.edgelist \\\n",
    "  --test_file experiment-data/bitcoin_alpha-test-1.edgelist \\\n",
    "  --device cuda \\\n",
    "  --node_feat_dim 16 \\\n",
    "  --embed_dim 16 \\\n",
    "  --num_heads 4 \\\n",
    "  --num_layers 2 \\\n",
    "  --dropout_rate 0.1 \\\n",
    "  --pretrain_path embeddings/bitcoin_alpha_ca_sdgnn_pretrained.pth \\\n",
    "  --finetune_path embeddings/bitcoin_alpha_ca_sdgnn_finetuned.pth \\\n",
    "  --output_dir output"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOy714LpHi2ILbfM9Oa67gO",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
